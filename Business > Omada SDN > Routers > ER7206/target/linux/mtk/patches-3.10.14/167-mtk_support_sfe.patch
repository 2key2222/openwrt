diff -ruN a/include/linux/skbuff.h b/include/linux/skbuff.h
--- a/include/linux/skbuff.h	2016-12-06 12:24:00.997678400 +0800
+++ b/include/linux/skbuff.h	2016-12-06 11:57:11.457335300 +0800
@@ -504,6 +504,7 @@
 	 * headers if needed
 	 */
 	__u8			encapsulation:1;
+	__u8			fast_forwarded:1;
 	/* 7/9 bit hole (depending on ndisc_nodetype presence) */
 	kmemcheck_bitfield_end(flags2);
 
diff -ruN a/net/bridge/br_if.c b/net/bridge/br_if.c
--- a/net/bridge/br_if.c	2014-09-02 16:42:59.000000000 +0800
+++ b/net/bridge/br_if.c	2016-12-07 08:35:26.112725445 +0800
@@ -463,3 +463,27 @@
 	rtnl_unlock();
 
 }
+/* Update bridge statistics for bridge packets processed by offload engines */
+void br_dev_update_stats(struct net_device *dev, struct rtnl_link_stats64 *nlstats)
+{
+	struct net_bridge *br;
+	struct br_cpu_netstats *stats;
+
+	/*
+	 * Is this a bridge?
+	 */
+	if (!(dev->priv_flags & IFF_EBRIDGE))
+		return;
+
+	br = netdev_priv(dev);
+	stats = per_cpu_ptr(br->stats, 0);
+
+	u64_stats_update_begin(&stats->syncp);
+	stats->rx_packets += nlstats->rx_packets;
+	stats->rx_bytes += nlstats->rx_bytes;
+	stats->tx_packets += nlstats->tx_packets;
+	stats->tx_bytes += nlstats->tx_bytes;
+	u64_stats_update_end(&stats->syncp);
+}
+EXPORT_SYMBOL_GPL(br_dev_update_stats);
+
diff -ruN a/net/core/dev.c b/net/core/dev.c
--- a/net/core/dev.c	2016-12-06 12:24:01.136678400 +0800
+++ b/net/core/dev.c	2016-12-06 12:11:22.158293700 +0800
@@ -2580,6 +2580,11 @@
 			}
 		}
 
+		/*
+		 * If this skb has been fast forwarded then we don't want it to
+		 * go to any taps (by definition we're trying to bypass them).
+		 */
+		if (unlikely(!skb->fast_forwarded)) {
 #if defined(CONFIG_IMQ) || defined(CONFIG_IMQ_MODULE)
 		if (!list_empty(&ptype_all) &&
 					!(skb->imq_flags & IMQ_F_ENQUEUE))
@@ -2587,7 +2592,7 @@
  		if (!list_empty(&ptype_all))
 #endif
  			dev_queue_xmit_nit(skb, dev);
- 
+} 
 
 		skb_len = skb->len;
 		rc = ops->ndo_start_xmit(skb, dev);
@@ -3424,6 +3429,8 @@
 	}
 }
 
+int (*athrs_fast_nat_recv)(struct sk_buff *skb) __rcu __read_mostly;
+EXPORT_SYMBOL_GPL(athrs_fast_nat_recv);
 static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
 {
 	struct packet_type *ptype, *pt_prev;
@@ -3433,6 +3440,7 @@
 	bool deliver_exact = false;
 	int ret = NET_RX_DROP;
 	__be16 type;
+	int (*fast_recv)(struct sk_buff *skb);
 
 	net_timestamp_check(!netdev_tstamp_prequeue, skb);
 
@@ -3464,6 +3472,13 @@
 		if (unlikely(!skb))
 			goto unlock;
 	}
+	fast_recv = rcu_dereference(athrs_fast_nat_recv);
+	if (fast_recv) {
+		if (fast_recv(skb)) {
+			ret = NET_RX_SUCCESS;
+			goto unlock;
+		}
+	}
 
 #ifdef CONFIG_NET_CLS_ACT
 	if (skb->tc_verd & TC_NCLS) {
diff -ruN a/net/netfilter/nf_conntrack_proto_tcp.c b/net/netfilter/nf_conntrack_proto_tcp.c
--- a/net/netfilter/nf_conntrack_proto_tcp.c	2014-09-02 16:43:02.000000000 +0800
+++ b/net/netfilter/nf_conntrack_proto_tcp.c	2016-12-06 12:14:13.439881400 +0800
@@ -31,10 +31,13 @@
 #include <net/netfilter/ipv4/nf_conntrack_ipv4.h>
 #include <net/netfilter/ipv6/nf_conntrack_ipv6.h>
 
+int nf_ct_tcp_no_window_check __read_mostly = 0;
+EXPORT_SYMBOL_GPL(nf_ct_tcp_no_window_check);
 /* "Be conservative in what you do,
     be liberal in what you accept from others."
     If it's non-zero, we mark only out of window RST segments as INVALID. */
-static int nf_ct_tcp_be_liberal __read_mostly = 0;
+int nf_ct_tcp_be_liberal __read_mostly = 0;
+EXPORT_SYMBOL_GPL(nf_ct_tcp_be_liberal);
 
 /* If it is set to zero, we disable picking up already established
    connections. */
@@ -528,6 +531,8 @@
 	s16 receiver_offset;
 	bool res;
 
+	if (nf_ct_tcp_no_window_check)
+		return true;
 	/*
 	 * Get the required data from the packet.
 	 */
@@ -1444,6 +1449,13 @@
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
 	},
+	{
+		.procname       = "nf_conntrack_tcp_no_window_check",
+		.data           = &nf_ct_tcp_no_window_check,
+		.maxlen         = sizeof(unsigned int),
+		.mode           = 0644,
+		.proc_handler   = proc_dointvec,
+	},
 	{ }
 };
 
