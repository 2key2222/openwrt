From df00a4028a5f82d7173122c70ade8be8b340a1fb Mon Sep 17 00:00:00 2001
From: Hanna Hawa <hannah@marvell.com>
Date: Sun, 12 Mar 2017 14:46:58 +0200
Subject: [PATCH 1463/2241] dma: mv_xor_v2: align DMA engine API mandates in
 submit function

The DMA engine API mandates that we should submit a descriptor to
a queue and then push them by invoking issue_pending.
This patch update the mv_xor_v2 driver to count the "ready" descriptors,
and push them to HW in issue_pending function.

Change-Id: I2ea602e5527398e4c2790f49d3c9b5be7a909247
Signed-off-by: Hanna Hawa <hannah@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/37414
Reviewed-by: Omri Itach <omrii@marvell.com>
Tested-by: Omri Itach <omrii@marvell.com>
---
 drivers/dma/mv_xor_v2.c | 50 +++++++++++++++++++++++++++----------------------
 1 file changed, 28 insertions(+), 22 deletions(-)

diff --git a/drivers/dma/mv_xor_v2.c b/drivers/dma/mv_xor_v2.c
index 4aa8510..ca340f8 100644
--- a/drivers/dma/mv_xor_v2.c
+++ b/drivers/dma/mv_xor_v2.c
@@ -154,6 +154,9 @@ struct mv_xor_v2_descriptor {
  * @hw_desq_virt: virtual address of DESCQ
  * @sw_desq: SW descriptors queue
  * @desc_size: HW descriptor size
+ * @npendings: number of pending descriptors (for which tx_submit has
+ * been called, but not yet issue_pending)
+ * @hw_queue_idx: index of next HW descriptor to push to the queue
 */
 struct mv_xor_v2_device {
 	spinlock_t sw_ll_lock;
@@ -171,6 +174,8 @@ struct mv_xor_v2_device {
 	struct mv_xor_v2_descriptor *hw_desq_virt;
 	struct mv_xor_v2_sw_desc *sw_desq;
 	int desc_size;
+	unsigned int npendings;
+	int hw_queue_idx;
 };
 
 /**
@@ -229,18 +234,6 @@ static void mv_xor_v2_set_data_buffers(struct mv_xor_v2_device *xor_dev,
 }
 
 /*
- * Return the next available index in the DESQ.
- */
-static inline int mv_xor_v2_get_desq_write_ptr(struct mv_xor_v2_device *xor_dev)
-{
-	/* read the index for the next available descriptor in the DESQ */
-	u32 reg = readl(xor_dev->dma_base + DMA_DESQ_ALLOC_OFF);
-
-	return ((reg >> DMA_DESQ_ALLOC_WRPTR_SHIFT)
-		& DMA_DESQ_ALLOC_WRPTR_MASK);
-}
-
-/*
  * notify the engine of new descriptors, and update the available index.
  */
 static void mv_xor_v2_add_desc_to_desq(struct mv_xor_v2_device *xor_dev,
@@ -329,7 +322,6 @@ static irqreturn_t mv_xor_v2_interrupt_handler(int irq, void *data)
 static dma_cookie_t
 mv_xor_v2_tx_submit(struct dma_async_tx_descriptor *tx)
 {
-	int desq_ptr;
 	void *dest_hw_desc;
 	dma_cookie_t cookie;
 	struct mv_xor_v2_sw_desc *sw_desc =
@@ -346,17 +338,19 @@ mv_xor_v2_tx_submit(struct dma_async_tx_descriptor *tx)
 	/* assign coookie */
 	cookie = dma_cookie_assign(tx);
 
-	/* get the next available slot in the DESQ */
-	desq_ptr = mv_xor_v2_get_desq_write_ptr(xor_dev);
-
 	/* copy the HW descriptor from the SW descriptor to the DESQ */
 	dest_hw_desc = ((void *)xor_dev->hw_desq_virt +
-			(xor_dev->desc_size * desq_ptr));
+			(xor_dev->desc_size * xor_dev->hw_queue_idx));
+	/*
+	 * Increase the push index for the HW queue and check if reach
+	 * the end of HW buffer
+	 */
+	if (++xor_dev->hw_queue_idx >= MV_XOR_V2_MAX_DESC_NUM)
+		xor_dev->hw_queue_idx = 0;
 
 	memcpy(dest_hw_desc, &sw_desc->hw_desc, xor_dev->desc_size);
 
-	/* update the DMA Engine with the new descriptor */
-	mv_xor_v2_add_desc_to_desq(xor_dev, 1);
+	xor_dev->npendings++;
 
 	/* unlock enqueue DESCQ */
 	spin_unlock_bh(&xor_dev->push_lock);
@@ -576,12 +570,22 @@ static enum dma_status mv_xor_v2_tx_status(struct dma_chan *chan,
 }
 
 /*
- * DMA framework requires this routine, since it's not
- * checking if device_issue_pending pointer is set or NULL
+ * issue all the pending descriptors (update the DMA Engine with
+ * the ready descriptors)
  */
 static void mv_xor_v2_issue_pending(struct dma_chan *chan)
 {
-	/* nothing to be done here */
+	struct mv_xor_v2_device *xor_dev =
+		container_of(chan, struct mv_xor_v2_device, dmachan);
+
+	/* Lock the channel */
+	spin_lock_bh(&xor_dev->push_lock);
+	if (xor_dev->npendings > 0) {
+		mv_xor_v2_add_desc_to_desq(xor_dev, xor_dev->npendings);
+		xor_dev->npendings = 0;
+	}
+	/* Release the channel */
+	spin_unlock_bh(&xor_dev->push_lock);
 }
 
 static inline
@@ -838,6 +842,8 @@ static int mv_xor_v2_probe(struct platform_device *pdev)
 		     (unsigned long) xor_dev);
 
 	xor_dev->desc_size = mv_xor_v2_set_desc_size(xor_dev);
+	xor_dev->npendings = 0;
+	xor_dev->hw_queue_idx = 0;
 
 	dma_cookie_init(&xor_dev->dmachan);
 
-- 
2.7.4

