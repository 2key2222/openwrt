diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c	2020-01-10 09:12:45.300847388 +0800
@@ -174,7 +174,6 @@
 
 	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
 	val |= MV_GMAC_PORT_CTRL2_CLK_125_BYPS_EN_MASK;
-	val &= ~MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
 	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
 
 	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
@@ -214,10 +213,6 @@
 	val &= ~MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
 	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
 
-	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
-	val &= ~MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
-	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
-
 	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
 	/* configure GIG MAC to SGMII mode */
 	val &= ~MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
@@ -256,7 +251,6 @@
 	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
 
 	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
-	val |= MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
 	val &= ~MV_GMAC_PORT_CTRL2_FC_MODE_MASK;
 	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
 
@@ -297,11 +291,89 @@
 	val |= MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
 	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
 
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	/* configure GIG MAC to 1000Base-X mode connected to a
+	 * fiber transceiver
+	 */
+	val &= ~MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* configure AN 0x9260 */
+	an = MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK    |
+		MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_AUTO_NEG_CFG_REG, an);
+}
+
+static void mv_gop110_gmac_1000basex_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 val, thresh, an;
+
+	/* configure minimal level of the Tx FIFO before the lower
+	 * part starts to read a packet
+	 */
+	thresh = MV_SGMII_TX_FIFO_MIN_TH;
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK,
+		      (thresh << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG, val);
+
+	/* Disable bypass of sync module */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL4_REG);
+	val |= MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK;
+	/* configure DP clock select according to mode */
+	val &= ~MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK;
+	/* configure QSGMII bypass according to mode */
+	val |= MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
+
 	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
-	val |= MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
+	val &= ~MV_GMAC_PORT_CTRL2_FC_MODE_MASK;
 	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
 
 	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	/* configure GIG MAC to 1000BASEX mode */
+	val |= MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* In 1000BaseX mode, we can't negotiate speed (it's
+	 * only 1000), and we do not want InBand autoneg
+	 * bypass enabled (link interrupt storm risk
+	 * otherwise).
+	 */
+	an = MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK     |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK    |
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_AUTO_NEG_CFG_REG, an);
+}
+
+static void mv_gop110_gmac_2500basex_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 val, thresh, an;
+
+	/* configure minimal level of the Tx FIFO before the lower
+	 * part starts to read a packet
+	 */
+	thresh = MV_SGMII2_5_TX_FIFO_MIN_TH;
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK,
+		      (thresh << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG, val);
+
+	/* Disable bypass of sync module */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL4_REG);
+	val |= MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK;
+	/* configure DP clock select according to mode */
+	val |= MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK;
+	/* configure QSGMII bypass according to mode */
+	val |= MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
 	/* configure GIG MAC to 1000Base-X mode connected to a
 	 * fiber transceiver
 	 */
@@ -917,8 +989,8 @@
 		/* select proper Mac mode */
 		mv_gop110_xlg_2_gig_mac_cfg(gop, mac_num);
 
-		/* pcs unreset */
-		mv_gop110_gpcs_reset(gop, mac_num, UNRESET);
+		/* set InBand AutoNeg */
+		mv_gop110_in_band_auto_neg(gop, mac_num, true);
 		/* mac unreset */
 		mv_gop110_gmac_reset(gop, mac_num, UNRESET);
 		mv_gop110_force_link_mode_set(gop, mac, false, false);
@@ -935,8 +1007,8 @@
 		/* select proper Mac mode */
 		mv_gop110_xlg_2_gig_mac_cfg(gop, mac_num);
 
-		/* pcs unreset */
-		mv_gop110_gpcs_reset(gop, mac_num, UNRESET);
+		/* set InBand AutoNeg */
+		mv_gop110_in_band_auto_neg(gop, mac_num, true);
 		/* mac unreset */
 		mv_gop110_gmac_reset(gop, mac_num, UNRESET);
 		mv_gop110_force_link_mode_set(gop, mac, false, false);
@@ -1025,8 +1097,8 @@
 	case PHY_INTERFACE_MODE_RGMII:
 	case PHY_INTERFACE_MODE_SGMII:
 	case PHY_INTERFACE_MODE_QSGMII:
-		/* pcs unreset */
-		mv_gop110_gpcs_reset(gop, mac_num, RESET);
+		/* set InBand AutoNeg */
+		mv_gop110_in_band_auto_neg(gop, mac_num, false);
 		/* mac unreset */
 		mv_gop110_gmac_reset(gop, mac_num, RESET);
 	break;
@@ -1125,6 +1197,7 @@
 	int port_num = mac->gop_index;
 
 	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
 	case PHY_INTERFACE_MODE_SGMII:
 	case PHY_INTERFACE_MODE_QSGMII:
 		mv_gop110_gmac_port_periodic_xon_set(gop, port_num, enable);
@@ -1641,24 +1714,7 @@
 	return 0;
 }
 
-/**************************************************************************
-* mv_gop110_gpcs_mode_cfg
-*
-* DESCRIPTION:
-	Configure port to working with Gig PCS or don't.
-*
-* INPUTS:
-*       pcs_num   - physical PCS number
-*       en        - true to enable PCS
-*
-* OUTPUTS:
-*       None.
-*
-* RETURNS:
-*       0  - on success
-*       1  - on error
-*
-**************************************************************************/
+/* Set GPCS mode configuration */
 int mv_gop110_gpcs_mode_cfg(struct gop_hw *gop, int pcs_num, bool en)
 {
 	u32 val;
@@ -1676,42 +1732,23 @@
 	return 0;
 }
 
-/**************************************************************************
-* mv_gop110_gpcs_reset
-*
-* DESCRIPTION:
-*       Set the selected PCS number to reset or exit from reset.
-*
-* INPUTS:
-*       pcs_num    - physical PCS number
-*       action    - reset / unreset
-*
-* OUTPUTS:
-*       None.
-*
-* RETURNS:
-*       0  - on success
-*       1  - on error
-*
-*************************************************************************/
-int  mv_gop110_gpcs_reset(struct gop_hw *gop, int pcs_num, enum mv_reset act)
+/* Set InBand AutoNeg configuration */
+int  mv_gop110_in_band_auto_neg(struct gop_hw *gop, int pcs_num, bool en)
 {
 	u32 reg_data;
 
 	reg_data = mv_gop110_gmac_read(gop, pcs_num, MV_GMAC_PORT_CTRL2_REG);
-	if (act == RESET)
-		U32_SET_FIELD(reg_data, MV_GMAC_PORT_CTRL2_SGMII_MODE_MASK, 0);
-	else
+	if (en)
 		U32_SET_FIELD(reg_data, MV_GMAC_PORT_CTRL2_SGMII_MODE_MASK,
 			      1 << MV_GMAC_PORT_CTRL2_SGMII_MODE_OFFS);
+	else
+		U32_SET_FIELD(reg_data, MV_GMAC_PORT_CTRL2_SGMII_MODE_MASK, 0);
 
 	mv_gop110_gmac_write(gop, pcs_num, MV_GMAC_PORT_CTRL2_REG, reg_data);
 	return 0;
 }
 
-/**************************************************************************
-* mv_gop110_smi_init
-**************************************************************************/
+/* Init SMI interface */
 int mv_gop110_smi_init(struct gop_hw *gop)
 {
 	u32 val;
@@ -1724,9 +1761,7 @@
 	return 0;
 }
 
-/**************************************************************************
-* mv_gop_phy_addr_cfg
-**************************************************************************/
+/* Set SMI PHY address */
 int mv_gop110_smi_phy_addr_cfg(struct gop_hw *gop, int port, int addr)
 {
 	mv_gop110_smi_write(gop, MV_SMI_PHY_ADDRESS_REG(port), addr);
@@ -2487,8 +2522,17 @@
 
 void mv_gop110_mib_counters_stat_update(struct gop_hw *gop, int port, struct gop_stat *gop_statistics)
 {
+	struct	mv_pp2x_hw *hw;
+	struct mv_pp2x *pp2;
+	struct mv_pp2x_port *pp_port;
+	unsigned long flags;
 	u64 val;
 
+	hw = container_of(gop, struct mv_pp2x_hw, gop);
+	pp2 = container_of(hw, struct mv_pp2x, hw);
+	pp_port = mv_pp2x_port_struct_get_by_gop_index(pp2, port);
+	spin_lock_irqsave(&pp_port->mac_data.stats_spinlock, flags);
+
 	gop_statistics->rx_byte += mv_gop110_mib_read64(gop, port,
 							MV_MIB_GOOD_OCTETS_RECEIVED_LOW);
 
@@ -2586,6 +2630,8 @@
 	/* This counter must be read last. Read it clear all the counters */
 	gop_statistics->late_collision += mv_gop110_mib_read64(gop, port,
 							MV_MIB_LATE_COLLISION);
+
+	spin_unlock_irqrestore(&pp_port->mac_data.stats_spinlock, flags);
 }
 
 void mv_gop110_mib_counters_clear(struct gop_hw *gop, int port)
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h	2020-01-08 09:34:17.000000000 +0800
@@ -171,7 +171,7 @@
 
 /* Gig PCS Functions */
 int mv_gop110_gpcs_mode_cfg(struct gop_hw *gop, int pcs_num, bool en);
-int mv_gop110_gpcs_reset(struct gop_hw *gop, int pcs_num, enum mv_reset act);
+int mv_gop110_in_band_auto_neg(struct gop_hw *gop, int pcs_num, bool en);
 
 /* MPCS Functions */
 
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c	2020-01-08 09:34:17.000000000 +0800
@@ -1190,3 +1190,35 @@
 {
 	netdev->ethtool_ops = &mv_pp2x_eth_tool_ops;
 }
+
+/* Following eth_tool_ops is for musdk_ports, i.e. eth_ports that have the musdk-status property in their dts. */
+static const struct ethtool_ops mv_pp2x_non_kernel_eth_tool_ops = {
+	.get_link		= ethtool_op_get_link,
+	.get_settings		= mv_pp2x_ethtool_get_settings,
+	/*.set_settings		= mv_pp2x_ethtool_set_settings,*/
+	/*.set_coalesce		= mv_pp2x_ethtool_set_coalesce,*/
+	/*.get_coalesce		= mv_pp2x_ethtool_get_coalesce,*/
+	.nway_reset		= mv_pp2x_eth_tool_nway_reset,
+	.get_drvinfo		= mv_pp2x_ethtool_get_drvinfo,
+	.get_ethtool_stats	= mv_pp2x_eth_tool_get_ethtool_stats,
+	.get_sset_count		= mv_pp2x_eth_tool_get_sset_count,
+	.get_strings		= mv_pp2x_eth_tool_get_strings,
+	/*.get_ringparam	= mv_pp2x_ethtool_get_ringparam,*/
+	/*.set_ringparam	= mv_pp2x_ethtool_set_ringparam,*/
+	.get_pauseparam		= mv_pp2x_get_pauseparam,
+	.set_pauseparam		= mv_pp2x_set_pauseparam,
+	.get_rxfh_indir_size	= mv_pp2x_ethtool_get_rxfh_indir_size,
+	.get_rxnfc		= mv_pp2x_ethtool_get_rxnfc,
+	.set_rxnfc		= mv_pp2x_ethtool_set_rxnfc,
+	.get_rxfh		= mv_pp2x_ethtool_get_rxfh,
+	.set_rxfh		= mv_pp2x_ethtool_set_rxfh,
+	.get_regs_len           = mv_pp2x_ethtool_get_regs_len,
+	.get_regs		= mv_pp2x_ethtool_get_regs,
+	.self_test		= mv_pp2x_eth_tool_diag_test,
+};
+
+void mv_pp2x_set_non_kernel_ethtool_ops(struct net_device *netdev)
+{
+	netdev->ethtool_ops = &mv_pp2x_non_kernel_eth_tool_ops;
+}
+
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h	2020-01-08 09:34:17.000000000 +0800
@@ -29,11 +29,34 @@
 #define MVPP2_DRIVER_NAME "mvpp2"
 #define MVPP2_DRIVER_VERSION "1.0"
 
+#define MVPP2X_SKB_MAGIC_MASK		0xFFFFFFC0
+#define MVPP2X_SKB_MAGIC_SKB_OFFS	3
+#define MVPP2X_SKB_PP2_CELL_OFFS	4
+#define MVPP2X_CB_REC_OFFS			5
+#define MVPP2X_SKB_BPID_MASK		0xF
+#define MVPP2X_SKB_CELL_MASK		0x3
+
+/* SKB magic, mainly used for skb recycle, here it is the address[34 : 8] of skb */
+#define MVPP2X_SKB_MAGIC(skb)   (((unsigned int)(((u64)skb) >> \
+				MVPP2X_SKB_MAGIC_SKB_OFFS)) & MVPP2X_SKB_MAGIC_MASK)
+/* Cb to store magic and bpid, IPv6 TCP will consume the most cb[] with 44 bytes, so the last 5 bytes is safe to use */
+#define MVPP2X_SKB_CB(skb)                          (*(unsigned int *)(&skb->cb[sizeof(skb->cb) - MVPP2X_CB_REC_OFFS]))
+/* Set magic and bpid, magic[31:5], pp2_id[5:4], source pool[3:0] */
+#define MVPP2X_SKB_MAGIC_BPID_SET(skb, magic_bpid)  (MVPP2X_SKB_CB(skb) = magic_bpid)
+/* Get bpid */
+#define MVPP2X_SKB_BPID_GET(skb)                    (MVPP2X_SKB_CB(skb) & MVPP2X_SKB_BPID_MASK)
+#define MVPP2X_SKB_PP2_CELL_GET(skb)        ((MVPP2X_SKB_CB(skb) >> MVPP2X_SKB_PP2_CELL_OFFS) & \
+						MVPP2X_SKB_CELL_MASK)
+
+#define MVPP2X_SKB_RECYCLE_MAGIC_GET(skb)           (MVPP2X_SKB_CB(skb) & MVPP2X_SKB_MAGIC_MASK)
+/* Recycle magic check */
+#define MVPP2X_SKB_RECYCLE_MAGIC_IS_OK(skb)         (MVPP2X_SKB_MAGIC(skb) == MVPP2X_SKB_RECYCLE_MAGIC_GET(skb))
+
 #define PFX			MVPP2_DRIVER_NAME ": "
 
 #define IRQ_NAME_SIZE (36)
 
-#define STATS_DELAY	250
+#define STATS_DELAY	1000 /*mSec*/
 
 #define TSO_TXQ_LIMIT 100
 #define TXQ_LIMIT (MAX_SKB_FRAGS + 2)
@@ -133,6 +156,7 @@
 /* Coalescing */
 #define MVPP2_TXDONE_COAL_PKTS		64
 #define MVPP2_TXDONE_HRTIMER_PERIOD_NS	1000000UL
+#define MVPP2_TX_HRTIMER_PERIOD_NS	50000UL
 #define MVPP2_TXDONE_COAL_USEC		1000
 
 #define MVPP2_RX_COAL_PKTS		32
@@ -150,6 +174,7 @@
 #define MVPP2_BM_SHORT_BUF_NUM		2048
 #define MVPP2_BM_LONG_BUF_NUM		1024
 #define MVPP2_BM_JUMBO_BUF_NUM		512
+#define MVPP2_BM_PER_CPU_THRESHOLD	(MVPP2_MAX_CPUS * 2)
 
 #define MVPP2_ALL_BUFS			0
 
@@ -164,9 +189,13 @@
 /* Used for define type of data saved in shadow: SKB or extended buffer or nothing */
 #define MVPP2_ETH_SHADOW_SKB		0x1
 #define MVPP2_ETH_SHADOW_EXT		0x2
+#define MVPP2_ETH_SHADOW_REC		0x4
+
+#define MVPP2_UNIQUE_HASH		0x4567492
 
 #define MVPP2_EXTRA_BUF_SIZE	120
 #define MVPP2_EXTRA_BUF_NUM	(MVPP2_MAX_TXD * MVPP2_MAX_TXQ)
+#define MVPP2_SKB_NUM		(MVPP2_MAX_RXD * MVPP2_MAX_RXQ * MVPP2_MAX_PORTS)
 
 enum mvppv2_version {
 	PPV21 = 21,
@@ -260,6 +289,9 @@
 	u32			link;
 	u32			duplex;
 	u32			speed;
+
+	/* Protect gop_statistics update by concurrent workqueue and ethtool */
+	spinlock_t		stats_spinlock;
 };
 
 /* Masks used for pp3_emac flags */
@@ -342,8 +374,11 @@
 	/* Number of Tx DMA descriptors in the descriptor ring */
 	int size;
 
-	/* Number of currently used Tx DMA descriptor in the descriptor ring */
-	int count;
+	/* Number of currently used Tx DMA descriptor in the descriptor ring used by SW */
+	int sw_count;
+
+	/* Number of currently used Tx DMA descriptor in the descriptor ring used by HW */
+	int hw_count;
 
 	/* Virtual pointer to address of the Aggr_Tx DMA descriptors
 	* memory_allocation
@@ -362,9 +397,9 @@
 	/* Index of the next Tx DMA descriptor to process */
 	int next_desc_to_proc;
 
-	/* Used to statistic the desc number to xmit in bulk */
-	u32 xmit_bulk;
-};
+	/* XPS mask */
+	cpumask_t affinity_mask;
+} __aligned(MVPP2_CACHE_LINE_SIZE);
 
 struct mv_pp2x_rx_queue {
 	/* RX queue number, in the range 0-31 for physical RXQs */
@@ -513,18 +548,24 @@
 
 	struct mv_pp2x_param_config pp2_cfg;
 
+	struct platform_device *pdev;
+
 	/* List of pointers to port structures */
-	u16 num_ports;
 	struct mv_pp2x_port **port_list;
+	u16 num_ports;
 
 	/* Aggregated TXQs */
 	u16 num_aggr_qs;
 	struct mv_pp2x_aggr_tx_queue *aggr_txqs;
+	u16 aggr_txqs_align_offs;
 
 	/* BM pools */
 	u16 num_pools;
 	struct mv_pp2x_bm_pool *bm_pools;
 
+	/* Per-CPU CP control */
+	struct mv_pp2x_cp_pcpu __percpu *pcpu;
+
 	/* RX flow hash indir'n table, in pp22, the table contains the
 	* CPU idx according to weight
 	*/
@@ -556,6 +597,17 @@
 	struct mv_pp2x_ext_buf_pool *ext_buf_pool;
 };
 
+/* Per-CPU CP control */
+struct mv_pp2x_cp_pcpu {
+	struct list_head skb_port_list;
+	struct mv_pp2x_skb_pool *skb_pool;
+	int in_use[MVPP2_BM_POOLS_NUM];
+
+	struct hrtimer tx_timer;
+	struct tasklet_struct tx_tasklet;
+	bool tx_timer_scheduled;
+};
+
 struct queue_vector {
 	u32 irq;
 	char irq_name[IRQ_NAME_SIZE];
@@ -634,6 +686,7 @@
 	struct mv_pp2x_cos cos_cfg;
 	struct mv_pp2x_rss rss_cfg;
 	struct notifier_block	port_hotplug_nb;
+	int use_interrupts;
 };
 
 struct pp2x_hw_params {
@@ -659,6 +712,11 @@
 	u8 *ext_buf_data;
 };
 
+struct mv_pp2x_skb_struct {
+	struct list_head skb_list;
+	struct sk_buff *skb;
+};
+
 struct mv_pp2x_ext_buf_pool {
 	int buf_pool_size;
 	int buf_pool_next_free;
@@ -666,6 +724,13 @@
 	struct mv_pp2x_ext_buf_struct *ext_buf_struct;
 };
 
+struct mv_pp2x_skb_pool {
+	int skb_pool_size;
+	int skb_pool_in_use;
+	int skb_pool_next_free;
+	struct mv_pp2x_skb_struct *skb_struct;
+};
+
 static inline struct mv_pp2x_port *mv_pp2x_port_struct_get(struct mv_pp2x *priv,
 							   int port)
 {
@@ -797,6 +862,7 @@
 int mv_pp2x_setup_txqs(struct mv_pp2x_port *port);
 void mv_pp2x_cleanup_txqs(struct mv_pp2x_port *port);
 void mv_pp2x_set_ethtool_ops(struct net_device *netdev);
+void mv_pp2x_set_non_kernel_ethtool_ops(struct net_device *netdev);
 int mv_pp22_rss_rxfh_indir_set(struct mv_pp2x_port *port);
 int mv_pp2x_cos_classifier_set(struct mv_pp2x_port *port,
 			       enum mv_pp2x_cos_classifier cos_mode);
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c	2020-01-08 09:34:17.000000000 +0800
@@ -411,16 +411,16 @@
 	/* Clear entry invalidation bit */
 	pe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] &= ~MVPP2_PRS_TCAM_INV_MASK;
 
-	/* Write tcam index - indirect access */
-	mv_pp2x_write(hw, MVPP2_PRS_TCAM_IDX_REG, pe->index);
-	for (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)
-		mv_pp2x_write(hw, MVPP2_PRS_TCAM_DATA_REG(i), pe->tcam.word[i]);
-
 	/* Write sram index - indirect access */
 	mv_pp2x_write(hw, MVPP2_PRS_SRAM_IDX_REG, pe->index);
 	for (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)
 		mv_pp2x_write(hw, MVPP2_PRS_SRAM_DATA_REG(i), pe->sram.word[i]);
 
+	/* Write tcam index - indirect access */
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_IDX_REG, pe->index);
+	for (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)
+		mv_pp2x_write(hw, MVPP2_PRS_TCAM_DATA_REG(i), pe->tcam.word[i]);
+
 	return 0;
 }
 EXPORT_SYMBOL(mv_pp2x_prs_hw_write);
@@ -920,22 +920,22 @@
 	mv_pp2x_prs_hw_write(hw, &pe);
 }
 
-/* Set port to promiscuous mode */
-void mv_pp2x_prs_mac_promisc_set(struct mv_pp2x_hw *hw, int port, bool add)
+/* Set port to unicast promiscuous mode */
+void mv_pp2x_prs_mac_uc_promisc_set(struct mv_pp2x_hw *hw, int port, bool add)
 {
 	struct mv_pp2x_prs_entry pe;
 
 	/* Promiscuous mode - Accept unknown packets */
 
-	if (hw->prs_shadow[MVPP2_PE_MAC_PROMISCUOUS].valid) {
+	if (hw->prs_shadow[MVPP2_PE_MAC_UC_PROMISCUOUS].valid) {
 		/* Entry exist - update port only */
-		pe.index = MVPP2_PE_MAC_PROMISCUOUS;
+		pe.index = MVPP2_PE_MAC_UC_PROMISCUOUS;
 		mv_pp2x_prs_hw_read(hw, &pe);
 	} else {
 		/* Entry doesn't exist - create new */
 		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
 		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
-		pe.index = MVPP2_PE_MAC_PROMISCUOUS;
+		pe.index = MVPP2_PE_MAC_UC_PROMISCUOUS;
 
 		/* Continue - set next lookup */
 		mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_DSA);
@@ -944,6 +944,10 @@
 		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L2_UCAST,
 					   MVPP2_PRS_RI_L2_CAST_MASK);
 
+		/* Update tcam entry data first byte */
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 0, MVPP2_PRS_UCAST_VAL,
+					       MVPP2_PRS_CAST_MASK);
+
 		/* Shift to ethertype */
 		mv_pp2x_prs_sram_shift_set(&pe, 2 * ETH_ALEN,
 					   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
@@ -961,27 +965,20 @@
 	mv_pp2x_prs_hw_write(hw, &pe);
 }
 
-/* Accept multicast */
-void mv_pp2x_prs_mac_multi_set(struct mv_pp2x_hw *hw, int port, int index,
-			       bool add)
+/* Set port to multicast promiscuous mode */
+void mv_pp2x_prs_mac_mc_promisc_set(struct mv_pp2x_hw *hw, int port, bool add)
 {
 	struct mv_pp2x_prs_entry pe;
-	unsigned char da_mc;
 
-	/* Ethernet multicast address first byte is
-	 * 0x01 for IPv4 and 0x33 for IPv6
-	 */
-	da_mc = (index == MVPP2_PE_MAC_MC_ALL) ? 0x01 : 0x33;
-
-	if (hw->prs_shadow[index].valid) {
+	if (hw->prs_shadow[MVPP2_PE_MAC_MC_PROMISCUOUS].valid) {
 		/* Entry exist - update port only */
-		pe.index = index;
+		pe.index = MVPP2_PE_MAC_MC_PROMISCUOUS;
 		mv_pp2x_prs_hw_read(hw, &pe);
 	} else {
 		/* Entry doesn't exist - create new */
 		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
 		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
-		pe.index = index;
+		pe.index = MVPP2_PE_MAC_MC_PROMISCUOUS;
 
 		/* Continue - set next lookup */
 		mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_DSA);
@@ -991,7 +988,8 @@
 					   MVPP2_PRS_RI_L2_CAST_MASK);
 
 		/* Update tcam entry data first byte */
-		mv_pp2x_prs_tcam_data_byte_set(&pe, 0, da_mc, 0xff);
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 0, MVPP2_PRS_MCAST_VAL,
+					       MVPP2_PRS_CAST_MASK);
 
 		/* Shift to ethertype */
 		mv_pp2x_prs_sram_shift_set(&pe, 2 * ETH_ALEN,
@@ -1035,10 +1033,6 @@
 		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);
 		pe.index = tid;
 
-		/* Shift 4 bytes if DSA tag or 8 bytes in case of EDSA tag*/
-		mv_pp2x_prs_sram_shift_set(&pe, shift,
-					   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
-
 		/* Update shadow table */
 		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_DSA);
 
@@ -1047,12 +1041,26 @@
 			mv_pp2x_prs_tcam_data_byte_set(&pe, 0,
 						       MVPP2_PRS_TCAM_DSA_TAGGED_BIT,
 					MVPP2_PRS_TCAM_DSA_TAGGED_BIT);
-			/* Clear all ai bits for next iteration */
-			mv_pp2x_prs_sram_ai_update(&pe, 0,
-						   MVPP2_PRS_SRAM_AI_MASK);
-			/* If packet is tagged continue check vlans */
-			mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);
+
+			/* Set ai bits for next iteration */
+			if (extend)
+				mv_pp2x_prs_sram_ai_update(&pe, 1,
+							   MVPP2_PRS_SRAM_AI_MASK);
+			else
+				mv_pp2x_prs_sram_ai_update(&pe, 0,
+							   MVPP2_PRS_SRAM_AI_MASK);
+
+			/* Set result info bits to 'single vlan' */
+			mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_SINGLE,
+						   MVPP2_PRS_RI_VLAN_MASK);
+
+			/* If packet is tagged continue check vid filtering */
+			mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VID);
 		} else {
+			/* Shift 4 bytes if DSA tag or 8 bytes in case of EDSA tag*/
+			mv_pp2x_prs_sram_shift_set(&pe, shift,
+						   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
 			/* Set result info bits to 'no vlans' */
 			mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,
 						   MVPP2_PRS_RI_VLAN_MASK);
@@ -1681,6 +1689,38 @@
 	mv_pp2x_prs_hw_write(hw, &pe);
 }
 
+/* Drop flow control pause frames */
+static void mv_pp2x_prs_drop_fc(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+	unsigned int len;
+	unsigned char da[ETH_ALEN] = {
+			0x01, 0x80, 0xC2, 0x00, 0x00, 0x01 };
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+
+	/* For all ports - drop flow control frames */
+	pe.index = MVPP2_PE_FC_DROP;
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
+
+	/* Set match on DA */
+	len = ETH_ALEN;
+	while (len--)
+		mv_pp2x_prs_tcam_data_byte_set(&pe, len, da[len], 0xff);
+
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DROP_MASK,
+				   MVPP2_PRS_RI_DROP_MASK);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MAC);
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
 /* Set default entires (place holder) for promiscuous, non-promiscuous and
  * multicast MAC addresses
  */
@@ -1707,11 +1747,10 @@
 	mv_pp2x_prs_hw_write(hw, &pe);
 
 	/* place holders only - no ports */
+	mv_pp2x_prs_drop_fc(hw);
 	mv_pp2x_prs_mac_drop_all_set(hw, 0, false);
-	mv_pp2x_prs_mac_promisc_set(hw, 0, false);
-
-	mv_pp2x_prs_mac_multi_set(hw, 0, MVPP2_PE_MAC_MC_ALL, false);
-	mv_pp2x_prs_mac_multi_set(hw, 0, MVPP2_PE_MAC_MC_IP6, false);
+	mv_pp2x_prs_mac_uc_promisc_set(hw, 0, false);
+	mv_pp2x_prs_mac_mc_promisc_set(hw, 0, false);
 }
 
 /* Set default entries for various types of dsa packets */
@@ -1991,8 +2030,8 @@
 
 /* Configure vlan entries and detect up to 2 successive VLAN tags.
  * Possible options:
- * 0x8100, 0x88A8
- * 0x8100, 0x8100
+ * 0x88A8, 0x8100 (outer, inner)
+ * 0x8100, 0x8100 (outer, inner)
  * 0x8100
  * 0x88A8
  */
@@ -2007,8 +2046,8 @@
 					    GFP_KERNEL);
 	if (!hw->prs_double_vlans)
 		return -ENOMEM;
-	/* Double VLAN: 0x8100, 0x88A8 */
-	err = mv_pp2x_prs_double_vlan_add(hw, ETH_P_8021Q, ETH_P_8021AD,
+	/* Double VLAN: 0x88A8, 0x8100 */
+	err = mv_pp2x_prs_double_vlan_add(hw, ETH_P_8021AD, ETH_P_8021Q,
 					  MVPP2_PRS_PORT_MASK);
 	if (err)
 		return err;
@@ -2084,8 +2123,41 @@
 	pe.index = MVPP2_PE_VID_FLTR_DEFAULT;
 	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_VID);
 
+	mv_pp2x_prs_tcam_ai_update(&pe, 0,
+				   MVPP2_PRS_EDSA_VID_AI_BIT);
+
 	/* Skip VLAN header - Set offset to 4 bytes */
-	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_VLAN_TAG_LEN, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_VLAN_TAG_LEN,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+	/* Clear all ai bits for next iteration */
+	mv_pp2x_prs_sram_ai_update(&pe, 0, MVPP2_PRS_SRAM_AI_MASK);
+
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_VID);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Set default vid entry for extended DSA*/
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+
+	/* Set default vid  entry */
+	pe.index = MVPP2_PE_VID_EDSA_FLTR_DEFAULT;
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_VID);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_EDSA_VID_AI_BIT,
+				   MVPP2_PRS_EDSA_VID_AI_BIT);
+
+	/* Skip VLAN header - Set offset to 8 bytes */
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_VLAN_TAG_EDSA_LEN,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+	/* Clear all ai bits for next iteration */
+	mv_pp2x_prs_sram_ai_update(&pe, 0, MVPP2_PRS_SRAM_AI_MASK);
 
 	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);
 
@@ -2095,6 +2167,7 @@
 	/* Update shadow table and hw entry */
 	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_VID);
 	mv_pp2x_prs_hw_write(hw, &pe);
+
 }
 
 /* Set entries for PPPoE ethertype */
@@ -2666,7 +2739,10 @@
 }
 
 /* Write parser entry for default VID filtering */
-static int mv_pp2x_prs_vid_drop_entry_accept(struct net_device *dev, unsigned int tid, bool add)
+static int mv_pp2x_prs_vid_drop_entry_accept(struct net_device *dev,
+					     unsigned int tid,
+					     unsigned int shift,
+					     bool add)
 {
 	struct mv_pp2x_prs_entry *pe;
 	unsigned int pmap;
@@ -2710,10 +2786,13 @@
 	/* Continue - set next lookup */
 	mv_pp2x_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_L2);
 
+	/* Skip VLAN header - Set offset to 4 or 8 bytes */
+	mv_pp2x_prs_sram_shift_set(pe, shift, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
 	mv_pp2x_prs_sram_ri_update(pe, MVPP2_PRS_RI_DROP_MASK, MVPP2_PRS_RI_DROP_MASK);
 
-	/* Skip VLAN header - Set offset to 4 bytes */
-	mv_pp2x_prs_sram_shift_set(pe, MVPP2_VLAN_TAG_LEN, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Clear all ai bits for next iteration */
+	mv_pp2x_prs_sram_ai_update(pe, 0, MVPP2_PRS_SRAM_AI_MASK);
 
 	/* Update shadow table */
 	mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_VID);
@@ -2784,6 +2863,7 @@
 	bool empty = false;
 	unsigned int pmap;
 	unsigned int mask = 0xfff;
+	unsigned int reg_val, shift;
 	struct mv_pp2x_port *port = netdev_priv(dev);
 	struct mv_pp2x_hw *hw = &port->priv->hw;
 	unsigned int vid_start = MVPP2_PE_VID_FILT_RANGE_START + port->id * MVPP2_PRS_VLAN_FILT_MAX;
@@ -2795,6 +2875,13 @@
 		/*no need to add vid 0 to HW*/
 		return 0;
 
+	/* Check configured start header */
+	reg_val = mv_pp2x_read(hw, MVPP2_MH_REG(port->id));
+	if (reg_val & MVPP2_DSA_EXTENDED)
+		shift = MVPP2_VLAN_TAG_EDSA_LEN;
+	else
+		shift = MVPP2_VLAN_TAG_LEN;
+
 	/* No such entry */
 	if (!pe) {
 		if (!add)
@@ -2802,7 +2889,9 @@
 
 		empty = mv_pp2x_prs_tcam_vid_empty(hw, vid_start, vid_start + MVPP2_PRS_VLAN_FILT_MAX_ENTRY);
 		if (empty) {
-			rc = mv_pp2x_prs_vid_drop_entry_accept(dev, vid_start + MVPP2_PRS_VLAN_FILT_DFLT_ENTRY, true);
+			rc = mv_pp2x_prs_vid_drop_entry_accept(dev,
+							       vid_start + MVPP2_PRS_VLAN_FILT_DFLT_ENTRY,
+							       shift, true);
 			if (rc) {
 				netdev_err(dev, "failed to add default vid entry for non-match vlan packets (drop)\n");
 				return rc;
@@ -2841,7 +2930,9 @@
 		hw->prs_shadow[pe->index].valid = false;
 		empty = mv_pp2x_prs_tcam_vid_empty(hw, vid_start, vid_start + MVPP2_PRS_VLAN_FILT_MAX_ENTRY);
 		if (empty) {
-			rc = mv_pp2x_prs_vid_drop_entry_accept(dev, vid_start + MVPP2_PRS_VLAN_FILT_DFLT_ENTRY, false);
+			rc = mv_pp2x_prs_vid_drop_entry_accept(dev,
+							       vid_start + MVPP2_PRS_VLAN_FILT_DFLT_ENTRY,
+							       shift, false);
 			if (rc) {
 				netdev_err(dev, "failed to remove default vid for non-match vlan packets (drop)\n");
 				return rc;
@@ -2854,11 +2945,14 @@
 	/* Continue - set next lookup */
 	mv_pp2x_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_L2);
 
+	/* Skip VLAN header - Set offset to 4 or 8 bytes */
+	mv_pp2x_prs_sram_shift_set(pe, shift, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
 	/* Set match on VID */
 	mv_pp2x_prs_match_vid(pe, MVPP2_PRS_VID_TCAM_BYTE, vid);
 
-	/* Skip VLAN header - Set offset to 4 bytes */
-	mv_pp2x_prs_sram_shift_set(pe, MVPP2_VLAN_TAG_LEN, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Clear all ai bits for next iteration */
+	mv_pp2x_prs_sram_ai_update(pe, 0, MVPP2_PRS_SRAM_AI_MASK);
 
 	/* Update shadow table */
 	mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_VID);
@@ -3270,9 +3364,9 @@
 				      MVPP2_CLS_LKP_HASH, MVPP2_CLS_FL_RSS_PRI);
 	fe->index = entry_idx;
 
-	/* Update last for UDP NF flow */
-	if ((lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT) &&
-	    !(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) {
+	/* Update last for TCP & UDP NF flow */
+	if (((lkpid_attr & (MVPP2_PRS_FL_ATTR_TCP_BIT | MVPP2_PRS_FL_ATTR_UDP_BIT)) &&
+	     !(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT))) {
 		if (!hw->cls_shadow->flow_info[lkpid -
 			MVPP2_PRS_FL_START].flow_entry_rss1) {
 			if (rss_mode == MVPP2_RSS_HASH_2T)
@@ -3331,8 +3425,9 @@
 					     MVPP2_COS_TYPE_DSCP);
 			/* RSS hash rule */
 			if ((!(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) &&
-			    (lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT)) {
-				/* RSS hash rules for UDP rss mode update */
+			    (lkpid_attr & (MVPP2_PRS_FL_ATTR_TCP_BIT |
+				MVPP2_PRS_FL_ATTR_UDP_BIT))) {
+				/* RSS hash rules for TCP & UDP rss mode update */
 				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
 							  MVPP2_RSS_HASH_2T);
 				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
@@ -3355,8 +3450,9 @@
 					     MVPP2_COS_TYPE_DSCP);
 			/* RSS hash rule */
 			if ((!(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) &&
-			    (lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT)) {
-				/* RSS hash rules for UDP rss mode update */
+			    (lkpid_attr & (MVPP2_PRS_FL_ATTR_TCP_BIT |
+				MVPP2_PRS_FL_ATTR_UDP_BIT))) {
+				/* RSS hash rules for TCP & UDP rss mode update */
 				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
 							  MVPP2_RSS_HASH_2T);
 				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
@@ -3892,14 +3988,14 @@
 				struct mv_pp2x_aggr_tx_queue *aggr_txq,
 				int num, int cpu)
 {
-	if ((aggr_txq->count + num) > aggr_txq->size) {
+	if ((aggr_txq->sw_count + aggr_txq->hw_count + num) > aggr_txq->size) {
 		/* Update number of occupied aggregated Tx descriptors */
 		u32 val = mv_pp2x_relaxed_read(&priv->hw,
 				MVPP2_AGGR_TXQ_STATUS_REG(cpu), cpu);
 
-		aggr_txq->count = val & MVPP2_AGGR_TXQ_PENDING_MASK;
+		aggr_txq->hw_count = val & MVPP2_AGGR_TXQ_PENDING_MASK;
 
-		if ((aggr_txq->count + num) > aggr_txq->size)
+		if ((aggr_txq->sw_count + aggr_txq->hw_count + num) > aggr_txq->size)
 			return -ENOMEM;
 	}
 
@@ -4151,6 +4247,12 @@
 	int tx_port_num = mv_pp2x_egress_port(port);
 	struct mv_pp2x_hw *hw = &port->priv->hw;
 
+	/* Don't do nothing for MUSDK ports since MUSDK manages
+	 * its own queues
+	 */
+	if (port->flags & MVPP2_F_IF_MUSDK)
+		return;
+
 	/* Enable all initialized TXs. */
 	qmap = 0;
 	for (queue = 0; queue < port->num_tx_queues; queue++) {
@@ -4176,6 +4278,12 @@
 	int tx_port_num = mv_pp2x_egress_port(port);
 	struct mv_pp2x_hw *hw = &port->priv->hw;
 
+	/* Don't do nothing for MUSDK ports since MUSDK manages
+	 * its own queues
+	 */
+	if (port->flags & MVPP2_F_IF_MUSDK)
+		return;
+
 	/* Issue stop command for active channels only */
 	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);
 	reg_data = (mv_pp2x_read(hw, MVPP2_TXP_SCHED_Q_CMD_REG)) &
@@ -6298,6 +6406,10 @@
 		return -EINVAL;
 
 	if (rss->sel == MVPP22_RSS_ACCESS_POINTER) {
+		/* Set index */
+		reg_val |= (rss->u.pointer.rxq_idx <<
+				MVPP22_RSS_IDX_RXQ_NUM_OFF);
+		mv_pp2x_write(hw, MVPP22_RSS_IDX_REG, reg_val);
 		/* Read entry */
 		rss->u.pointer.rss_tbl_ptr =
 			mv_pp2x_read(hw, MVPP22_RSS_RXQ2RSS_TBL_REG) &
@@ -6428,6 +6540,9 @@
 {
 	struct mv_pp2x_hw *hw = &port->priv->hw;
 	int val, queue;
+	unsigned long flags;
+
+	spin_lock_irqsave(&port->mac_data.stats_spinlock, flags);
 
 	val = mv_pp2x_read(hw, MV_PP2_OVERRUN_DROP_REG(port->id));
 	gop_statistics->rx_ppv2_overrun += val;
@@ -6438,7 +6553,6 @@
 	gop_statistics->rx_cls_drop += val;
 	gop_statistics->rx_hw_drop += val;
 
-	preempt_disable();
 	for (queue = port->first_rxq; queue < (port->first_rxq +
 			port->num_rx_queues); queue++) {
 		mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, queue);
@@ -6454,7 +6568,8 @@
 		gop_statistics->rx_bm_drop += val;
 		gop_statistics->rx_hw_drop += val;
 	}
-	preempt_enable();
+
+	spin_unlock_irqrestore(&port->mac_data.stats_spinlock, flags);
 }
 
 /*  Clear Mvpp2x counter statistic */
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h	2020-01-08 09:34:17.000000000 +0800
@@ -185,7 +185,8 @@
 	u32 val;
 
 	val = MVPP2_CAUSE_MISC_SUM_MASK | MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK;
-	if (port->priv->pp2xdata->interrupt_tx_done)
+	/* Don't unmask Tx done interrupts for ports working in Netmap mode*/
+	if (!(port->flags & MVPP2_F_IFCAP_NETMAP) && port->priv->pp2xdata->interrupt_tx_done)
 		val |= MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;
 
 	mv_pp2x_write(&port->priv->hw,
@@ -292,6 +293,11 @@
 					       dma_addr_t buf_phys_addr,
 					      u8 *buf_virt_addr, int cpu)
 {
+#if defined(CONFIG_ARCH_DMA_ADDR_T_64BIT) && defined(CONFIG_PHYS_ADDR_T_64BIT)
+	mv_pp2x_relaxed_write(hw, MVPP22_BM_PHY_VIRT_HIGH_RLS_REG,
+			      upper_32_bits(buf_phys_addr), cpu);
+#endif
+
 	mv_pp2x_relaxed_write(hw, MVPP2_BM_VIRT_RLS_REG,
 			      lower_32_bits((uintptr_t)buf_virt_addr), cpu);
 
@@ -530,9 +536,8 @@
 
 int mv_pp2x_prs_default_init(struct platform_device *pdev,
 			     struct mv_pp2x_hw *hw);
-void mv_pp2x_prs_mac_promisc_set(struct mv_pp2x_hw *hw, int port, bool add);
-void mv_pp2x_prs_mac_multi_set(struct mv_pp2x_hw *hw, int port, int index,
-			       bool add);
+void mv_pp2x_prs_mac_uc_promisc_set(struct mv_pp2x_hw *hw, int port, bool add);
+void mv_pp2x_prs_mac_mc_promisc_set(struct mv_pp2x_hw *hw, int port, bool add);
 int mv_pp2x_prs_mac_da_accept(struct mv_pp2x_port *port,
 			      const u8 *da, bool add);
 int mv_pp2x_prs_vid_entry_accept(struct net_device *dev, u16 proto, u16 vid, bool add);
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h	2020-01-08 09:34:17.000000000 +0800
@@ -38,7 +38,6 @@
 /*TODO*/
 /*AXI_BRIDGE*/
 /*AXI_CONTEXT*/
-/*Top Regfile*/
 
 #define MVPP21_DESC_ADDR_SHIFT		0 /*Applies to RXQ, AGGR_TXQ*/
 #define MVPP22_DESC_ADDR_SHIFT		(9 - 1) /*Applies to RXQ, AGGR_TXQ*/
@@ -157,6 +156,18 @@
 #define MVPP22_AXI_CODE_DOMAIN_SYSTEM		3
 #define MVPP22_AXI_CODE_DOMAIN_NON_SHARE	0
 
+/* Top Reg file */
+#define MVPP2_MH_REG(port)			(0x5040 + 4 * (port))
+
+#define MVPP2_MH_EN_OFFS			0
+#define MVPP2_MH_EN_MASK			BIT(MVPP2_MH_EN_OFFS)
+
+#define MVPP2_DSA_EN_OFFS			4
+#define MVPP2_DSA_EN_MASK			(0x3 << MVPP2_DSA_EN_OFFS)
+#define MVPP2_DSA_DISABLE			0
+#define MVPP2_DSA_NON_EXTENDED			(0x1 << MVPP2_DSA_EN_OFFS)
+#define MVPP2_DSA_EXTENDED			(0x2 << MVPP2_DSA_EN_OFFS)
+
 /* Parser Registers */
 #define MVPP2_PRS_INIT_LOOKUP_REG		0x1000
 #define MVPP2_PRS_PORT_LU_MAX			0xf
@@ -1167,10 +1178,14 @@
 #define MVPP2_ETH_TYPE_LEN		2
 #define MVPP2_PPPOE_HDR_SIZE		8
 #define MVPP2_VLAN_TAG_LEN		4
+#define MVPP2_VLAN_TAG_EDSA_LEN		8
 
 /* Lbtd 802.3 type */
 #define MVPP2_IP_LBDT_TYPE		0xfffa
 
+#define MVPP2_CACHE_LINE_SIZE		L1_CACHE_BYTES
+#define MVPP2_CACHE_LINE_MASK		(L1_CACHE_BYTES - 1)
+
 #define MVPP2_CPU_D_CACHE_LINE_SIZE	32
 #define MVPP2_TX_CSUM_MAX_SIZE		9800
 
@@ -1214,7 +1229,7 @@
 #define MVPP2_CPU_DESC_CHUNK		128
 
 /* Max number of Tx descriptors in each aggregated queue */
-#define MVPP2_AGGR_TXQ_SIZE		512
+#define MVPP2_AGGR_TXQ_SIZE		2048
 
 /* Descriptor aligned size */
 #define MVPP2_DESC_ALIGNED_SIZE		32
@@ -1224,8 +1239,8 @@
 					MVPP2_DESC_Q_ALIGN)
 #define MVPP2_DESCQ_MEM_ALIGN(mem)	(ALIGN(mem, MVPP2_DESC_Q_ALIGN))
 
-/* Descriptor alignment mask */
-#define MVPP2_TX_DESC_ALIGN		(MVPP2_DESC_ALIGNED_SIZE - 1)
+/* TX descriptor data offset mask */
+#define MVPP2_TX_DESC_DATA_OFFSET		0XFF
 
 /* TX FIFO constants */
 #define MVPP2_TX_FIFO_DATA_SIZE_10KB		0xa
@@ -1269,8 +1284,10 @@
 #define MVPP2_MAX_L3_ADDR_SIZE		16
 
 /* Port flags */
-#define MVPP2_F_LOOPBACK		BIT(0)
-#define MVPP2_F_IFCAP_NETMAP    BIT(1)
+#define MVPP2_F_LOOPBACK		BIT(0) /* Loopback port */
+#define MVPP2_F_IFCAP_NETMAP		BIT(1) /* netmap port */
+#define MVPP2_F_IF_MUSDK		BIT(2) /* musdk port */
+#define MVPP2_F_IF_MUSDK_DOWN		BIT(3) /* musdk port that has been put stopped */
 
 /* Marvell tag types */
 enum mv_pp2x_tag_type {
@@ -1304,6 +1321,9 @@
 #define MVPP2_PRS_TCAM_PROTO_MASK	0xff
 #define MVPP2_PRS_TCAM_PROTO_MASK_L	0x3f
 #define MVPP2_PRS_DBL_VLANS_MAX		100
+#define MVPP2_PRS_CAST_MASK		0x1
+#define MVPP2_PRS_MCAST_VAL		0x1
+#define MVPP2_PRS_UCAST_VAL		0x0
 
 /* There is a TCAM range reserved for MAC entries, range size is 80
  * 1 BC MAC entry for all ports
@@ -1357,29 +1377,30 @@
 #define MVPP2_PE_VID_FILT_RANGE_START	(MVPP2_PE_VID_FILT_RANGE_END - MVPP2_PRS_VLAN_FILT_RANGE_SIZE + 1)
 #define MVPP2_PE_LAST_FREE_TID		(MVPP2_PE_MAC_RANGE_START - 1)
 #define MVPP2_PE_IP6_EXT_PROTO_UN	(MVPP2_PRS_TCAM_SRAM_SIZE - 30)
-#define MVPP2_PE_MAC_MC_IP6		(MVPP2_PRS_TCAM_SRAM_SIZE - 29)
-#define MVPP2_PE_IP6_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 28)
-#define MVPP2_PE_IP4_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 27)
-#define MVPP2_PE_LAST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 26)
-#define MVPP2_PE_FIRST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 20)
-#define MVPP2_PE_EDSA_TAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 19)
-#define MVPP2_PE_EDSA_UNTAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 18)
-#define MVPP2_PE_DSA_TAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 17)
-#define MVPP2_PE_DSA_UNTAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 16)
-#define MVPP2_PE_ETYPE_EDSA_TAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 15)
-#define MVPP2_PE_ETYPE_EDSA_UNTAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 14)
-#define MVPP2_PE_ETYPE_DSA_TAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 13)
-#define MVPP2_PE_ETYPE_DSA_UNTAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 12)
-#define MVPP2_PE_MH_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 11)
-#define MVPP2_PE_DSA_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 10)
-#define MVPP2_PE_IP6_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 9)
-#define MVPP2_PE_IP4_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 8)
-#define MVPP2_PE_ETH_TYPE_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 7)
-#define MVPP2_PE_VID_FLTR_DEFAULT	(MVPP2_PRS_TCAM_SRAM_SIZE - 6)
-#define MVPP2_PE_VLAN_DBL		(MVPP2_PRS_TCAM_SRAM_SIZE - 5)
-#define MVPP2_PE_VLAN_NONE		(MVPP2_PRS_TCAM_SRAM_SIZE - 4)
-#define MVPP2_PE_MAC_MC_ALL		(MVPP2_PRS_TCAM_SRAM_SIZE - 3)
-#define MVPP2_PE_MAC_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 2)
+#define MVPP2_PE_IP6_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 29)
+#define MVPP2_PE_IP4_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 28)
+#define MVPP2_PE_LAST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 27)
+#define MVPP2_PE_FIRST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 22)
+#define MVPP2_PE_EDSA_TAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 21)
+#define MVPP2_PE_EDSA_UNTAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 20)
+#define MVPP2_PE_DSA_TAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 19)
+#define MVPP2_PE_DSA_UNTAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 18)
+#define MVPP2_PE_ETYPE_EDSA_TAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 17)
+#define MVPP2_PE_ETYPE_EDSA_UNTAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 16)
+#define MVPP2_PE_ETYPE_DSA_TAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 15)
+#define MVPP2_PE_ETYPE_DSA_UNTAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 14)
+#define MVPP2_PE_MH_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 13)
+#define MVPP2_PE_DSA_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 12)
+#define MVPP2_PE_IP6_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 11)
+#define MVPP2_PE_IP4_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 10)
+#define MVPP2_PE_ETH_TYPE_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 9)
+#define MVPP2_PE_VID_FLTR_DEFAULT	(MVPP2_PRS_TCAM_SRAM_SIZE - 8)
+#define MVPP2_PE_VID_EDSA_FLTR_DEFAULT	(MVPP2_PRS_TCAM_SRAM_SIZE - 7)
+#define MVPP2_PE_VLAN_DBL		(MVPP2_PRS_TCAM_SRAM_SIZE - 6)
+#define MVPP2_PE_VLAN_NONE		(MVPP2_PRS_TCAM_SRAM_SIZE - 5)
+#define MVPP2_PE_FC_DROP		(MVPP2_PRS_TCAM_SRAM_SIZE - 4)
+#define MVPP2_PE_MAC_MC_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 3)
+#define MVPP2_PE_MAC_UC_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 2)
 #define MVPP2_PE_MAC_NON_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 1)
 
 /* Sram structure
@@ -1476,6 +1497,7 @@
 #define MVPP2_PRS_IPV6_EXT_AH_L4_AI_BIT		BIT(4)
 #define MVPP2_PRS_SINGLE_VLAN_AI		0
 #define MVPP2_PRS_DBL_VLAN_AI_BIT		BIT(7)
+#define MVPP2_PRS_EDSA_VID_AI_BIT		BIT(0)
 
 #define MVPP2_PRS_SRAM_SHIFT_MASK		((1 << \
 					MVPP2_PRS_SRAM_SHIFT_BITS) - 1)
@@ -1793,6 +1815,7 @@
 
 #define MVPP2_TXD_L3_OFF_SHIFT		0
 #define MVPP2_TXD_IP_HLEN_SHIFT		8
+#define MVPP2_TXD_BUF_MOD		BIT(7)
 #define MVPP2_TXD_L4_CSUM_FRAG		BIT(13)
 #define MVPP2_TXD_L4_CSUM_NOT		BIT(14)
 #define MVPP2_TXD_IP_CSUM_DISABLE	BIT(15)
diff -Nuar a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c	2017-11-16 11:10:56.000000000 +0800
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c	2020-01-10 09:14:01.900847677 +0800
@@ -271,6 +271,14 @@
 		ext_buf_pool->buf_pool_next_free++;
 }
 
+static void mv_pp2x_skb_pool_inc(struct mv_pp2x_skb_pool *skb_pool)
+{
+	if (unlikely(skb_pool->skb_pool_next_free == skb_pool->skb_pool_size - 1))
+		skb_pool->skb_pool_next_free = 0;
+	else
+		skb_pool->skb_pool_next_free++;
+}
+
 static u8 mv_pp2x_first_pool_get(struct mv_pp2x *priv)
 {
 	return priv->pp2_cfg.first_bm_pool;
@@ -315,6 +323,15 @@
 {
 	dma_addr_t phys_addr;
 	void *data;
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+
+	/* BM pool is refilled only if number of used buffers is bellow
+	 * refill threshold. Number of used buffers could be decremented
+	 * by recycling mechanism.
+	 */
+	if (is_recycle &&
+	    (cp_pcpu->in_use[bm_pool->id] < bm_pool->in_use_thresh))
+		return 0;
 
 	data = mv_pp2x_frag_alloc(bm_pool);
 	if (!data)
@@ -330,6 +347,11 @@
 	}
 
 	mv_pp2x_pool_refill(port->priv, pool, phys_addr, cpu);
+
+	/* Decrement only if refill called from RX */
+	if (is_recycle)
+		cp_pcpu->in_use[bm_pool->id]--;
+
 	return 0;
 }
 
@@ -514,7 +536,7 @@
 	dev_err(&pdev->dev, "failed to create BM pool %d, size %d\n", i, size);
 	for (i = i - 1; i >= 0; i--)
 		mv_pp2x_bm_pool_destroy(&pdev->dev, priv, &priv->bm_pools[i]);
-		return err;
+	return err;
 }
 
 static int mv_pp2x_bm_init(struct platform_device *pdev, struct mv_pp2x *priv)
@@ -578,7 +600,7 @@
 
 	/* Update BM driver with number of buffers added to pool */
 	bm_pool->buf_num += i;
-	bm_pool->in_use_thresh = bm_pool->buf_num / 4;
+	bm_pool->in_use_thresh = bm_pool->buf_num / MVPP2_BM_PER_CPU_THRESHOLD;
 
 	netdev_dbg(port->dev,
 		   "%s pool %d: pkt_size=%4d, buf_size=%4d, total_size=%4d\n",
@@ -737,6 +759,7 @@
 	int pkt_size = MVPP2_RX_PKT_SIZE(mtu);
 
 	old_long_pool = old_long_port_pool->log_id;
+	old_short_pool = old_short_port_pool->log_id;
 
 	/* If port MTU is higher than 1518B:
 	* HW Long pool - SW Jumbo pool, HW Short pool - SW Short pool
@@ -751,6 +774,13 @@
 	}
 
 	if (new_long_pool != old_long_pool) {
+		/* Remove port from old short&long pool */
+		mv_pp2x_bm_pool_stop_use(port, old_long_pool);
+		old_long_port_pool->port_map &= ~(1 << port->id);
+
+		mv_pp2x_bm_pool_stop_use(port, old_short_pool);
+		old_short_port_pool->port_map &= ~(1 << port->id);
+
 		/* Add port to new short&long pool */
 		port->pool_long = mv_pp2x_bm_pool_use(port, new_long_pool);
 		if (!port->pool_long)
@@ -768,13 +798,6 @@
 			port->priv->pp2xdata->mv_pp2x_rxq_short_pool_set(hw,
 			port->rxqs[rxq]->id, port->pool_short->id);
 
-		/* Remove port from old short&long pool */
-		mv_pp2x_bm_pool_stop_use(port, old_long_pool);
-		old_long_port_pool->port_map &= ~(1 << port->id);
-
-		mv_pp2x_bm_pool_stop_use(port, old_short_pool);
-		old_short_port_pool->port_map &= ~(1 << port->id);
-
 		/* Update L4 checksum when jumbo enable/disable on port */
 		if (new_long_pool == MVPP2_BM_SWF_JUMBO_POOL) {
 			if (port->id != port->priv->l4_chksum_jumbo_port) {
@@ -850,6 +873,11 @@
 		mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(queue), val);
 	}
 
+	/* Enable classifier high queue in forwarding port control*/
+	val = mv_pp2x_read(hw, MVPP2_CLS_SWFWD_PCTRL_REG);
+	val &= ~(MVPP2_CLS_SWFWD_PCTRL_MASK(port->id));
+	mv_pp2x_write(hw, MVPP2_CLS_SWFWD_PCTRL_REG, val);
+
 	/* At default, mask all interrupts to all present cpus */
 	mv_pp2x_port_interrupts_disable(port);
 }
@@ -898,6 +926,51 @@
 	return 0;
 }
 
+static inline struct sk_buff *mv_pp2_skb_pool_get(struct mv_pp2x_port *port)
+{
+	struct sk_buff *skb;
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	struct mv_pp2x_skb_struct *skb_struct;
+
+	if (!list_empty(&cp_pcpu->skb_port_list)) {
+		skb_struct = list_last_entry(&cp_pcpu->skb_port_list,
+					     struct mv_pp2x_skb_struct, skb_list);
+		list_del(&skb_struct->skb_list);
+		cp_pcpu->skb_pool->skb_pool_in_use--;
+
+		skb = skb_struct->skb;
+
+	} else {
+		skb = NULL;
+	}
+
+	return skb;
+}
+
+static inline int mv_pp2_skb_pool_put(struct mv_pp2x_port *port, struct sk_buff *skb,
+				      int cpu)
+{
+	struct mv_pp2x_cp_pcpu *cp_pcpu = per_cpu_ptr(port->priv->pcpu, cpu);
+	struct mv_pp2x_skb_struct *skb_struct;
+
+	if (cp_pcpu->skb_pool->skb_pool_in_use >= cp_pcpu->skb_pool->skb_pool_size) {
+		dev_kfree_skb_any(skb);
+		return 1;
+	}
+	cp_pcpu->skb_pool->skb_pool_in_use++;
+
+	skb_struct =
+		&cp_pcpu->skb_pool->skb_struct[cp_pcpu->skb_pool->skb_pool_next_free];
+	mv_pp2x_skb_pool_inc(cp_pcpu->skb_pool);
+
+	skb_struct->skb = skb;
+
+	list_add(&skb_struct->skb_list,
+		 &cp_pcpu->skb_port_list);
+
+	return 0;
+}
+
 /* Check if there are enough reserved descriptors for transmission.
  * If not, request chunk of reserved descriptors and check again.
  */
@@ -999,17 +1072,46 @@
 				    txq_pcpu->tx_buffs[txq_pcpu->txq_get_index];
 		uintptr_t skb = (uintptr_t)txq_pcpu->tx_skb[txq_pcpu->txq_get_index];
 		int data_size = txq_pcpu->data_size[txq_pcpu->txq_get_index];
+		struct sk_buff *skb_rec;
 
-		dma_unmap_single(port->dev->dev.parent, buf_phys_addr,
-				 data_size, DMA_TO_DEVICE);
+		txq_pcpu->tx_buffs[txq_pcpu->txq_get_index] = 0;
+		txq_pcpu->tx_skb[txq_pcpu->txq_get_index] = 0;
+		txq_pcpu->data_size[txq_pcpu->txq_get_index] = 0;
 
 		if (skb & MVPP2_ETH_SHADOW_EXT) {
+			/* Refill TSO external pool */
 			skb &= ~MVPP2_ETH_SHADOW_EXT;
 			mv_pp2_extra_pool_put(port, (void *)skb, txq_pcpu->cpu);
 			mv_pp2x_txq_inc_get(txq_pcpu);
+			dma_unmap_single(port->dev->dev.parent, buf_phys_addr,
+					 data_size, DMA_TO_DEVICE);
+			continue;
+		} else if (skb & MVPP2_ETH_SHADOW_REC) {
+			/* Release skb without data buffer, if data buffer were marked as
+			 * recycled in TX routine.
+			 */
+			struct mv_pp2x_bm_pool *bm_pool;
+			struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+
+			skb &= ~MVPP2_ETH_SHADOW_REC;
+			skb_rec = (struct sk_buff *)skb;
+			bm_pool = &port->priv->bm_pools[MVPP2X_SKB_BPID_GET(skb_rec)];
+			cp_pcpu->in_use[bm_pool->id]--;
+			/* Do not release buffer of recycled skb */
+			if (!skb_irq_freeable(skb_rec)) {
+				skb_rec->head = NULL;
+				dev_kfree_skb_any(skb_rec);
+			} else {
+				skb_rec->head = NULL;
+				mv_pp2_skb_pool_put(port, skb_rec, txq_pcpu->cpu);
+			}
+			mv_pp2x_txq_inc_get(txq_pcpu);
 			continue;
 		}
 
+		dma_unmap_single(port->dev->dev.parent, buf_phys_addr,
+				 data_size, DMA_TO_DEVICE);
+
 		if (skb & MVPP2_ETH_SHADOW_SKB) {
 			skb &= ~MVPP2_ETH_SHADOW_SKB;
 			dev_kfree_skb_any((struct sk_buff *)skb);
@@ -1191,6 +1293,8 @@
 	int rx_received, i, cpu;
 	u8 *buf_cookie;
 	dma_addr_t buf_phys_addr;
+	struct mv_pp2x_bm_pool *bm_pool;
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
 
 	preempt_disable();
 	rx_received = mv_pp2x_rxq_received(port, rxq->id);
@@ -1203,6 +1307,15 @@
 		struct mv_pp2x_rx_desc *rx_desc =
 			mv_pp2x_rxq_next_desc_get(rxq);
 
+#if defined(__BIG_ENDIAN)
+		if (port->priv->pp2_version == PPV21)
+			mv_pp21_rx_desc_swap(rx_desc);
+		else
+			mv_pp22_rx_desc_swap(rx_desc);
+#endif /* __BIG_ENDIAN */
+
+		bm_pool = &port->priv->bm_pools[MVPP2_RX_DESC_POOL(rx_desc)];
+
 		if (port->priv->pp2_version == PPV21)
 			buf_phys_addr = mv_pp21_rxdesc_phys_addr_get(rx_desc);
 		else
@@ -1212,6 +1325,7 @@
 
 		mv_pp2x_pool_refill(port->priv, MVPP2_RX_DESC_POOL(rx_desc),
 				    buf_phys_addr, cpu);
+		cp_pcpu->in_use[bm_pool->id]--;
 	}
 	put_cpu();
 	mv_pp2x_rxq_status_update(port, rxq->id, rx_received, rx_received);
@@ -1314,18 +1428,18 @@
 	for_each_present_cpu(cpu) {
 		txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
 		txq_pcpu->size = txq->size;
-		txq_pcpu->tx_skb = kmalloc(txq_pcpu->size *
+		txq_pcpu->tx_skb = kcalloc(txq_pcpu->size,
 					   sizeof(*txq_pcpu->tx_skb),
 					   GFP_KERNEL);
 		if (!txq_pcpu->tx_skb)
 			goto error;
 
-		txq_pcpu->tx_buffs = kmalloc(txq_pcpu->size *
+		txq_pcpu->tx_buffs = kcalloc(txq_pcpu->size,
 					     sizeof(dma_addr_t), GFP_KERNEL);
 		if (!txq_pcpu->tx_buffs)
 			goto error;
 
-		txq_pcpu->data_size = kmalloc(txq_pcpu->size *
+		txq_pcpu->data_size = kcalloc(txq_pcpu->size,
 						sizeof(int), GFP_KERNEL);
 		if (!txq_pcpu->data_size)
 			goto error;
@@ -1346,7 +1460,7 @@
 	}
 
 	dma_free_coherent(port->dev->dev.parent,
-			  txq->size * MVPP2_DESC_ALIGNED_SIZE,
+			  MVPP2_DESCQ_MEM_SIZE(txq->size),
 			  txq->first_desc, txq->descs_phys);
 
 	return -ENOMEM;
@@ -1838,6 +1952,28 @@
 	}
 }
 
+/* Set transmit TX timer */
+static void mv_pp2x_tx_timer_set(struct mv_pp2x_cp_pcpu *cp_pcpu)
+{
+	ktime_t interval;
+
+	if (!cp_pcpu->tx_timer_scheduled) {
+		cp_pcpu->tx_timer_scheduled = true;
+		interval = ktime_set(0, MVPP2_TX_HRTIMER_PERIOD_NS);
+		hrtimer_start(&cp_pcpu->tx_timer, interval,
+			      HRTIMER_MODE_REL_PINNED);
+	}
+}
+
+/* Cancel transmit TX timer */
+static inline void mv_pp2x_tx_timer_kill(struct mv_pp2x_cp_pcpu *cp_pcpu)
+{
+	if (cp_pcpu->tx_timer_scheduled) {
+		cp_pcpu->tx_timer_scheduled = false;
+		hrtimer_cancel(&cp_pcpu->tx_timer);
+	}
+}
+
 static void mv_pp2x_tx_proc_cb(unsigned long data)
 {
 	struct net_device *dev = (struct net_device *)data;
@@ -1858,6 +1994,36 @@
 		mv_pp2x_timer_set(port_pcpu);
 }
 
+/* TX to HW the pendings in aggregated TXQ; kill deferring TX hrtimer */
+static inline void mv_pp2x_aggr_txq_pend_send(struct mv_pp2x_port *port,
+					      struct mv_pp2x_cp_pcpu *cp_pcpu,
+					      struct mv_pp2x_aggr_tx_queue *aggr_txq)
+{
+	mv_pp2x_tx_timer_kill(cp_pcpu);
+	aggr_txq->hw_count += aggr_txq->sw_count;
+	mv_pp2x_write(&port->priv->hw, MVPP2_AGGR_TXQ_UPDATE_REG, aggr_txq->sw_count);
+	aggr_txq->sw_count = 0;
+}
+
+/* Tasklet transmit procedure */
+static void mv_pp2x_tx_send_proc_cb(unsigned long data)
+{
+	struct mv_pp2x *priv = (struct mv_pp2x *)data;
+	struct mv_pp2x_aggr_tx_queue *aggr_txq;
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(priv->pcpu);
+	int cpu = smp_processor_id();
+
+	cp_pcpu->tx_timer_scheduled = false;
+
+	aggr_txq = &priv->aggr_txqs[cpu];
+
+	if (likely(aggr_txq->sw_count > 0)) {
+		aggr_txq->hw_count += aggr_txq->sw_count;
+		mv_pp2x_write(&priv->hw, MVPP2_AGGR_TXQ_UPDATE_REG, aggr_txq->sw_count);
+		aggr_txq->sw_count = 0;
+	}
+}
+
 static enum hrtimer_restart mv_pp2x_hr_timer_cb(struct hrtimer *timer)
 {
 	struct mv_pp2x_port_pcpu *port_pcpu = container_of(timer,
@@ -1868,6 +2034,16 @@
 	return HRTIMER_NORESTART;
 }
 
+static enum hrtimer_restart mv_pp2x_tx_hr_timer_cb(struct hrtimer *timer)
+{
+	struct mv_pp2x_cp_pcpu *cp_pcpu = container_of(timer,
+			 struct mv_pp2x_cp_pcpu, tx_timer);
+
+	tasklet_schedule(&cp_pcpu->tx_tasklet);
+
+	return HRTIMER_NORESTART;
+}
+
 /* The function calculate the width, such as cpu width, cos queue width */
 static void mv_pp2x_width_calc(struct mv_pp2x_port *port, u32 *cpu_width,
 			       u32 *cos_width, u32 *port_rxq_width)
@@ -2170,8 +2346,8 @@
 		lkpid = index + MVPP2_PRS_FL_START;
 		/* Get lookup ID attribute */
 		lkpid_attr = mv_pp2x_prs_flow_id_attr_get(lkpid);
-		/* Only non-frag UDP can set rss mode */
-		if ((lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT) &&
+		/* Only non-frag TCP & UDP can set rss mode */
+		if ((lkpid_attr & (MVPP2_PRS_FL_ATTR_TCP_BIT | MVPP2_PRS_FL_ATTR_UDP_BIT)) &&
 		    !(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) {
 			/* Prepare a temp table for the lkpid */
 			mv_pp2x_cls_flow_tbl_temp_copy(hw, lkpid, &flow_idx);
@@ -2342,13 +2518,47 @@
 static void mv_pp2x_set_skb_hash(struct mv_pp2x_rx_desc *rx_desc, u32 rx_status,
 				 struct sk_buff *skb)
 {
-	u32 hash;
-
-	hash = (u32)(rx_desc->u.pp22.buf_phys_addr_key_hash >> 40);
+	/* Store unique Marvell hash to indicate recycled skb.
+	*  Hash is used as additional to skb->cb protection
+	*  for recycling. skb->hash unused in xmit procedure since
+	*  .ndo_select_queue callback implemented in driver.
+	*/
 	if ((rx_status & MVPP2_RXD_L4_UDP) || (rx_status & MVPP2_RXD_L4_TCP))
-		skb_set_hash(skb, hash, PKT_HASH_TYPE_L4);
+		skb_set_hash(skb, MVPP2_UNIQUE_HASH, PKT_HASH_TYPE_L4);
 	else
-		skb_set_hash(skb, hash, PKT_HASH_TYPE_L3);
+		skb_set_hash(skb, MVPP2_UNIQUE_HASH, PKT_HASH_TYPE_L3);
+}
+
+/* Function is similar to build_skb routine without sbk kmem_cache_alloc */
+static void mv_pp2x_build_skb(struct sk_buff *skb, unsigned char *data,
+			      unsigned int frag_size)
+{
+	struct skb_shared_info *shinfo;
+	unsigned int size = frag_size ? : ksize(data);
+
+	size -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->truesize = SKB_TRUESIZE(size);
+	atomic_set(&skb->users, 1);
+	skb->head = data;
+	skb->data = data;
+	skb_reset_tail_pointer(skb);
+	skb->end = skb->tail + size;
+	skb->mac_header = (typeof(skb->mac_header))~0U;
+	skb->transport_header = (typeof(skb->transport_header))~0U;
+
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+	kmemcheck_annotate_variable(shinfo->destructor_arg);
+
+	if (skb && frag_size) {
+		skb->head_frag = 1;
+		if (page_is_pfmemalloc(virt_to_head_page(data)))
+			skb->pfmemalloc = 1;
+	}
 }
 
 /* Main rx processing */
@@ -2363,12 +2573,13 @@
 	u8  num_pool = MVPP2_BM_SWF_NUM_POOLS;
 	u8  first_bm_pool = port->priv->pp2_cfg.first_bm_pool;
 	int cpu = smp_processor_id();
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
 
 #ifdef DEV_NETMAP
 		if (port->flags & MVPP2_F_IFCAP_NETMAP) {
 			int netmap_done = 0;
 
-			if (netmap_rx_irq(port->dev, 0, &netmap_done))
+			if (netmap_rx_irq(port->dev, rxq->log_id, &netmap_done))
 				return netmap_done;
 		/* Netmap implementation includes all queues in i/f.*/
 		return 1;
@@ -2437,9 +2648,19 @@
 			mv_pp2x_pool_refill(port->priv, pool, buf_phys_addr, cpu);
 			continue;
 		}
+		/* Try to get skb from CP skb pool
+		*  If get func return skb -> use mv_pp2x_build_skb to reset skb
+		*  else -> use regular build_skb callback
+		*/
+		skb = mv_pp2_skb_pool_get(port);
 
-		skb = build_skb(data, bm_pool->frag_size > PAGE_SIZE ? 0 :
+		if (skb)
+			mv_pp2x_build_skb(skb, data, bm_pool->frag_size > PAGE_SIZE ? 0 :
+				bm_pool->frag_size);
+		else
+			skb = build_skb(data, bm_pool->frag_size > PAGE_SIZE ? 0 :
 				bm_pool->frag_size);
+
 		if (unlikely(!skb)) {
 			netdev_warn(port->dev, "skb build failed\n");
 			goto err_drop_frame;
@@ -2449,6 +2670,7 @@
 				 MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),
 				 DMA_FROM_DEVICE);
 		refill_array[bm_pool->log_id]++;
+		cp_pcpu->in_use[bm_pool->id]++;
 
 #ifdef MVPP2_VERBOSE
 		mv_pp2x_skb_dump(skb, rx_desc->data_size, 4);
@@ -2466,6 +2688,10 @@
 
 		if (likely(dev->features & NETIF_F_RXCSUM))
 			mv_pp2x_rx_csum(port, rx_status, skb);
+		/* Store skb magic id sequence for recycling  */
+		MVPP2X_SKB_MAGIC_BPID_SET(skb, (MVPP2X_SKB_MAGIC(skb) |
+					(port->priv->pp2_cfg.cell_index << 4) |
+							pool));
 
 		skb_record_rx_queue(skb, (u16)rxq->log_id);
 		mv_pp2x_set_skb_hash(rx_desc, rx_status, skb);
@@ -2485,7 +2711,7 @@
 		refill_bm_pool = &port->priv->bm_pools[i];
 		while (likely(refill_array[i]--)) {
 			err = mv_pp2x_rx_refill_new(port, refill_bm_pool,
-						    refill_bm_pool->id, 0, cpu);
+						    refill_bm_pool->id, true, cpu);
 			if (unlikely(err)) {
 				netdev_err(port->dev, "failed to refill BM pools\n");
 				refill_array[i]++;
@@ -2548,9 +2774,9 @@
 			mv_pp2x_txq_desc_put(txq);
 			goto error;
 		}
-		tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;
+		tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_DATA_OFFSET;
 		mv_pp2x_txdesc_phys_addr_set(port->priv->pp2_version,
-					     buf_phys_addr & ~MVPP2_TX_DESC_ALIGN, tx_desc);
+					     buf_phys_addr & ~MVPP2_TX_DESC_DATA_OFFSET, tx_desc);
 
 		if (i == (skb_shinfo(skb)->nr_frags - 1)) {
 			/* Last descriptor */
@@ -2691,10 +2917,10 @@
 		return -1;
 	}
 
-	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;
+	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_DATA_OFFSET;
 
 	mv_pp2x_txdesc_phys_addr_set(port->priv->pp2_version,
-				     buf_phys_addr & ~MVPP2_TX_DESC_ALIGN, tx_desc);
+				     buf_phys_addr & ~MVPP2_TX_DESC_DATA_OFFSET, tx_desc);
 
 	mv_pp2x_txq_inc_put(port->priv->pp2_version,
 			    txq_pcpu,
@@ -2726,10 +2952,10 @@
 		return -1;
 	}
 
-	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;
+	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_DATA_OFFSET;
 
 	mv_pp2x_txdesc_phys_addr_set(port->priv->pp2_version,
-				     buf_phys_addr & ~MVPP2_TX_DESC_ALIGN, tx_desc);
+				     buf_phys_addr & ~MVPP2_TX_DESC_DATA_OFFSET, tx_desc);
 
 	tx_desc->command = 0;
 
@@ -2764,6 +2990,7 @@
 	u32 tcp_seq = 0;
 	skb_frag_t *skb_frag_ptr;
 	const struct tcphdr *th = tcp_hdr(skb);
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
 
 	if (unlikely(mv_pp2_tso_validate(skb, dev)))
 		return 0;
@@ -2887,22 +3114,21 @@
 		}
 	}
 
-	aggr_txq->xmit_bulk += total_desc_num;
+	aggr_txq->sw_count += total_desc_num;
+
 	if (!skb->xmit_more) {
-		/* Transmit TCP segment with bulked descriptors*/
-		mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->xmit_bulk);
-		aggr_txq->xmit_bulk = 0;
+		/* Transmit TCP segment with bulked descriptors and cancel tx hr timer if exist */
+		mv_pp2x_aggr_txq_pend_send(port, cp_pcpu, aggr_txq);
 	}
 
 	txq_pcpu->reserved_num -= total_desc_num;
-	aggr_txq->count += total_desc_num;
 
 	return total_desc_num;
 
 out_no_tx_desc:
 	/* No enough memory for packet header - rollback */
 	pr_err("%s: No TX descriptors - rollback %d, txq_count=%d, nr_frags=%d, skb=%p, len=%d, gso_segs=%d\n",
-	       __func__, total_desc_num, aggr_txq->count, skb_shinfo(skb)->nr_frags,
+	       __func__, total_desc_num, (aggr_txq->hw_count + aggr_txq->sw_count), skb_shinfo(skb)->nr_frags,
 			skb, skb->len, skb_shinfo(skb)->gso_segs);
 
 	for (i = 0; i < total_desc_num; i++) {
@@ -2925,6 +3151,63 @@
 	return 0;
 }
 
+/* Routine to check if skb recyclable. Data buffer cannot be recycled if:
+ * 1. skb is cloned, shared, nonlinear, zero copied.
+ * 2. skb with not align data buffer.
+ * 3. IRQs are disabled.
+ */
+static inline bool mv_pp2x_skb_is_recycleable(const struct sk_buff *skb, int skb_size)
+{
+	if (unlikely(irqs_disabled()))
+		return false;
+
+	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY))
+		return false;
+
+	if (unlikely(skb_is_nonlinear(skb) || skb->fclone != SKB_FCLONE_UNAVAILABLE))
+		return false;
+
+	skb_size = SKB_DATA_ALIGN(skb_size + NET_SKB_PAD);
+	if (unlikely(skb_end_pointer(skb) - skb->head < skb_size))
+		return false;
+
+	if (unlikely(skb_shared(skb) || skb_cloned(skb)))
+		return false;
+
+	return true;
+}
+
+/* Routine to get BM pool from skb cb */
+static inline struct mv_pp2x_bm_pool *mv_pp2x_skb_recycle_get_pool(struct mv_pp2x *priv, struct sk_buff *skb)
+{
+	if (MVPP2X_SKB_RECYCLE_MAGIC_IS_OK(skb))
+		return &priv->bm_pools[MVPP2X_SKB_BPID_GET(skb)];
+	else
+		return NULL;
+}
+
+/* Routine:
+ * 1. Check that it's save to recycle skb by mv_pp2x_skb_is_recycleable routine
+ * 2. Check if MVPP2 unique hash were set in RX routine.
+ * 3. Check that recycle is on same CPN.
+ * 4. Test skb->cb magic and return BM pool ID if its pass all criterions.
+ * Otherwise -1 returned.
+ */
+static inline int mv_pp2x_skb_recycle_check(struct mv_pp2x *priv, struct sk_buff *skb)
+{
+	struct mv_pp2x_bm_pool *bm_pool;
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(priv->pcpu);
+
+	if ((skb->hash == MVPP2_UNIQUE_HASH) && (MVPP2X_SKB_PP2_CELL_GET(skb) == priv->pp2_cfg.cell_index)) {
+		bm_pool = mv_pp2x_skb_recycle_get_pool(priv, skb);
+		if (bm_pool)
+			if (mv_pp2x_skb_is_recycleable(skb, bm_pool->pkt_size) && (cp_pcpu->in_use[bm_pool->id] > 0))
+				return bm_pool->id;
+	}
+
+	return -1;
+}
+
 /* Main tx processing */
 static int mv_pp2x_tx(struct sk_buff *skb, struct net_device *dev)
 {
@@ -2935,10 +3218,12 @@
 	struct netdev_queue *nq;
 	struct mv_pp2x_tx_desc *tx_desc;
 	dma_addr_t buf_phys_addr;
-	int frags = 0;
+	int frags = 0, pool_id;
 	u16 txq_id;
 	u32 tx_cmd;
 	int cpu = smp_processor_id();
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	u8 recycling;
 
 	/* Set relevant physical TxQ and Linux netdev queue */
 	txq_id = skb_get_queue_mapping(skb) % mv_pp2x_txq_number;
@@ -3017,19 +3302,32 @@
 	}
 	pr_debug("buf_phys_addr=%x\n", (unsigned int)buf_phys_addr);
 
-	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;
+	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_DATA_OFFSET;
 	mv_pp2x_txdesc_phys_addr_set(port->priv->pp2_version,
-				     buf_phys_addr & ~MVPP2_TX_DESC_ALIGN, tx_desc);
+				     buf_phys_addr & ~MVPP2_TX_DESC_DATA_OFFSET, tx_desc);
 
 	tx_cmd = mv_pp2x_skb_tx_csum(port, skb);
+
 	if (frags == 1) {
 		/* First and Last descriptor */
+		/* Check if skb should be recycled */
+		pool_id = mv_pp2x_skb_recycle_check(port->priv, skb);
+		/* If pool ID provided -> packet should be recycled.
+		*  Set recycled field in TX descriptor and add skb recycle shadow.
+		*/
+		if (pool_id > -1) {
+			tx_cmd |= MVPP2_TXD_BUF_MOD;
+			tx_cmd |= ((pool_id << MVPP2_RXD_BM_POOL_ID_OFFS) & MVPP2_RXD_BM_POOL_ID_MASK);
+			recycling = MVPP2_ETH_SHADOW_REC;
+		} else {
+			recycling = MVPP2_ETH_SHADOW_SKB;
+		}
 
 		tx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_L_DESC;
 		tx_desc->command = tx_cmd;
 		mv_pp2x_txq_inc_put(port->priv->pp2_version,
 				    txq_pcpu, (struct sk_buff *)((uintptr_t)skb |
-			MVPP2_ETH_SHADOW_SKB), tx_desc);
+					recycling), tx_desc);
 	} else {
 		/* First but not Last */
 		tx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_PADDING_DISABLE;
@@ -3046,19 +3344,21 @@
 		}
 	}
 	txq_pcpu->reserved_num -= frags;
-	aggr_txq->count += frags;
-	aggr_txq->xmit_bulk += frags;
+	aggr_txq->sw_count += frags;
 
 #ifdef CONFIG_MV_PTP_SERVICE
 	/* If packet is PTP add Time-Stamp request into the tx_desc */
 	mv_pp2_is_pkt_ptp_tx_proc(port, tx_desc, skb);
 #endif
 
-	/* Enable transmit */
+	/* Start 50 microseconds timer to transmit */
 	if (!skb->xmit_more) {
-		mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->xmit_bulk);
-		aggr_txq->xmit_bulk = 0;
+		if (skb->hash == MVPP2_UNIQUE_HASH)
+			mv_pp2x_tx_timer_set(cp_pcpu);
+		else
+			mv_pp2x_aggr_txq_pend_send(port, cp_pcpu, aggr_txq);
 	}
+
 out:
 	if (likely(frags > 0)) {
 		struct mv_pp2x_pcpu_stats *stats = this_cpu_ptr(port->stats);
@@ -3069,15 +3369,18 @@
 		u64_stats_update_end(&stats->syncp);
 	} else {
 		/* Transmit bulked descriptors*/
-		if (aggr_txq->xmit_bulk > 0) {
-			mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->xmit_bulk);
-			aggr_txq->xmit_bulk = 0;
-		}
+		if (aggr_txq->sw_count > 0)
+			mv_pp2x_aggr_txq_pend_send(port, cp_pcpu, aggr_txq);
 		dev->stats.tx_dropped++;
 		dev_kfree_skb_any(skb);
 	}
 	/* PPV22 TX Post-Processing */
 
+#ifdef DEV_NETMAP
+	/* Don't check tx done for ports working in Netmap mode */
+	if ((port->flags & MVPP2_F_IFCAP_NETMAP))
+		return NETDEV_TX_OK;
+#endif
 	if (!port->priv->pp2xdata->interrupt_tx_done)
 		mv_pp2x_tx_done_post_proc(txq, txq_pcpu, port, frags);
 
@@ -3125,16 +3428,8 @@
 
 #ifdef DEV_NETMAP
 	if ((port->flags & MVPP2_F_IFCAP_NETMAP)) {
-		u32 state;
-
-		state = mv_pp2x_qvector_interrupt_state_get(q_vec);
-		if (state)
-			mv_pp2x_qvector_interrupt_disable(q_vec);
-		cause_rx = 0;
 		napi_complete(napi);
-		if (state)
-			mv_pp2x_qvector_interrupt_enable(q_vec);
-		q_vec->pending_cause_rx = cause_rx;
+		q_vec->pending_cause_rx = 0;
 		return rx_done;
 	}
 #endif
@@ -3376,16 +3671,20 @@
 	if (port->priv->pp2_version == PPV21) {
 		mv_pp21_port_enable(port);
 	} else {
-		mv_gop110_port_events_mask(gop, mac);
-		mv_gop110_port_enable(gop, mac, port->comphy);
+		if (!(port->flags & MVPP2_F_LOOPBACK)) {
+			mv_gop110_port_events_mask(gop, mac);
+			mv_gop110_port_enable(gop, mac, port->comphy);
+		}
 	}
 
 	if (port->mac_data.phy_dev) {
 		phy_start(port->mac_data.phy_dev);
 	} else {
-		mv_pp22_dev_link_event(port->dev);
-		tasklet_init(&port->link_change_tasklet, mv_pp2_link_change_tasklet,
-			     (unsigned long)(port->dev));
+		if (!(port->flags & MVPP2_F_LOOPBACK)) {
+			mv_pp22_dev_link_event(port->dev);
+			tasklet_init(&port->link_change_tasklet, mv_pp2_link_change_tasklet,
+				     (unsigned long)(port->dev));
+		}
 	}
 
 	if (port->mac_data.phy_dev)
@@ -3394,31 +3693,64 @@
 	mv_pp2x_egress_enable(port);
 	mv_pp2x_ingress_enable(port);
 	/* Unmask link_event */
-	if (port->priv->pp2_version == PPV22) {
+	if (port->priv->pp2_version == PPV22 && !(port->flags & MVPP2_F_LOOPBACK)) {
 		mv_gop110_port_events_unmask(gop, mac);
 		port->mac_data.flags |= MV_EMAC_F_PORT_UP;
 	}
 }
 
+/* Drain pending packets */
+static void mv_pp2x_send_pend_aggr_txq(void *arg)
+{
+	struct mv_pp2x_port *port = arg;
+	struct mv_pp2x_aggr_tx_queue *aggr_txq =
+		&port->priv->aggr_txqs[smp_processor_id()];
+	int txq_id;
+	struct mv_pp2x_tx_queue *txq;
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
+	bool free_aggr = false;
+
+	for (txq_id = 0; txq_id < port->num_tx_queues; txq_id++) {
+		txq = port->txqs[txq_id];
+		txq_pcpu = this_cpu_ptr(txq->pcpu);
+		if (mv_pp2x_txq_free_count(txq_pcpu) != txq->size) {
+			free_aggr = true;
+			break;
+		}
+	}
+
+	if (!aggr_txq->sw_count || !free_aggr)
+		return; /* no pendings */
+
+	/* Schedule Drain over the same tasklet-context
+	 * which the regular TX is using (refer mv_pp2x_tx_send_proc_cb).
+	 * So the Drain from the stop_dev and TX are unpreemptive and correct.
+	 */
+	cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	tasklet_schedule(&cp_pcpu->tx_tasklet);
+}
+
 /* Set hw internals when stopping port */
 void mv_pp2x_stop_dev(struct mv_pp2x_port *port)
 {
 	struct gop_hw *gop = &port->priv->hw.gop;
 	struct mv_mac_data *mac = &port->mac_data;
 
-	/* Stop new packets from arriving to RXQs */
+	/* Stop new packets arriving from RX-interrupts and Linux-TX */
 	mv_pp2x_ingress_disable(port);
+	netif_carrier_off(port->dev);
+	msleep(20);
 
-	mdelay(10);
+	/* Drain pending aggregated TXQ on all CPUs */
+	on_each_cpu(mv_pp2x_send_pend_aggr_txq, port, 1);
+	msleep(200); /* yield and wait for tx-tasklet and HW idle */
 
 	/* Disable interrupts on all CPUs */
 	mv_pp2x_port_interrupts_disable(port);
 
 	mv_pp2x_port_napi_disable(port);
-
-	netif_carrier_off(port->dev);
 	netif_tx_stop_all_queues(port->dev);
-
 	mv_pp2x_egress_disable(port);
 
 	if (port->comphy)
@@ -3427,16 +3759,19 @@
 	if (port->priv->pp2_version == PPV21) {
 		mv_pp21_port_disable(port);
 	} else {
-		mv_gop110_port_events_mask(gop, mac);
-		mv_gop110_port_disable(gop, mac, port->comphy);
-		port->mac_data.flags &= ~MV_EMAC_F_LINK_UP;
-		port->mac_data.flags &= ~MV_EMAC_F_PORT_UP;
+		if (!(port->flags & MVPP2_F_LOOPBACK)) {
+			mv_gop110_port_events_mask(gop, mac);
+			mv_gop110_port_disable(gop, mac, port->comphy);
+			port->mac_data.flags &= ~MV_EMAC_F_LINK_UP;
+			port->mac_data.flags &= ~MV_EMAC_F_PORT_UP;
+		}
 	}
 
 	if (port->mac_data.phy_dev)
 		phy_stop(port->mac_data.phy_dev);
 	else
-		tasklet_kill(&port->link_change_tasklet);
+		if (!(port->flags & MVPP2_F_LOOPBACK))
+			tasklet_kill(&port->link_change_tasklet);
 }
 
 /* Return positive if MTU is valid */
@@ -3662,6 +3997,11 @@
 	struct mv_pp2x_port *port = netdev_priv(dev);
 	int err;
 
+	if (port->flags & MVPP2_F_IF_MUSDK_DOWN) {
+		netdev_warn(dev, "skipping ndo_open as this port isn't really down\n");
+		return 0;
+	}
+
 	set_device_base_address(dev);
 
 	/* Allocate the Rx/Tx queues */
@@ -3675,7 +4015,6 @@
 		netdev_err(port->dev, "cannot allocate Tx queues\n");
 		goto err_cleanup_rxqs;
 	}
-
 	err = mv_pp2x_setup_irqs(dev, port);
 	if (err) {
 		netdev_err(port->dev, "cannot allocate irq's\n");
@@ -3683,21 +4022,22 @@
 	}
 
 	/* Only Mvpp22 support hot plug feature */
-	if (port->priv->pp2_version == PPV22)
+	if (port->priv->pp2_version == PPV22  && !(port->flags & (MVPP2_F_IF_MUSDK | MVPP2_F_LOOPBACK)))
 		register_hotcpu_notifier(&port->port_hotplug_nb);
 
 	/* In default link is down */
 	netif_carrier_off(port->dev);
 
-	/* Unmask interrupts on all CPUs */
-	on_each_cpu(mv_pp2x_interrupts_unmask, port, 1);
+	if (!(port->flags & MVPP2_F_IF_MUSDK)) {
+		/* Unmask interrupts on all CPUs */
+		on_each_cpu(mv_pp2x_interrupts_unmask, port, 1);
 
-	/* Unmask shared interrupts */
-	mv_pp2x_shared_thread_interrupts_unmask(port);
+		/* Unmask shared interrupts */
+		mv_pp2x_shared_thread_interrupts_unmask(port);
 
-	/* Port is init in uboot */
-
-	if (port->priv->pp2_version == PPV22)
+		/* Port is init in uboot */
+	}
+	if ((port->priv->pp2_version == PPV22) && !(port->flags & MVPP2_F_LOOPBACK))
 		mvcpn110_mac_hw_init(port);
 	mv_pp2x_start_dev(port);
 
@@ -3736,8 +4076,8 @@
 
 	if (port->priv->pp2_version == PPV22)
 		unregister_hotcpu_notifier(&port->port_hotplug_nb);
-
-	if (!port->priv->pp2xdata->interrupt_tx_done) {
+	/* Cancel tx timers in case Tx done interrupts are disabled and if port is not in Netmap mode */
+	if (!(port->flags & MVPP2_F_IFCAP_NETMAP) && !port->priv->pp2xdata->interrupt_tx_done)  {
 		for_each_present_cpu(cpu) {
 			port_pcpu = per_cpu_ptr(port->pcpu, cpu);
 			hrtimer_cancel(&port_pcpu->tx_done_timer);
@@ -3757,11 +4097,10 @@
 	struct mv_pp2x_hw *hw = &port->priv->hw;
 	int id = port->id;
 
-	/* Accept all: Multicast + Unicast */
-	mv_pp2x_prs_mac_multi_set(hw, id, MVPP2_PE_MAC_MC_ALL, true);
-	mv_pp2x_prs_mac_multi_set(hw, id, MVPP2_PE_MAC_MC_IP6, true);
 	/* Enter promisc mode */
-	mv_pp2x_prs_mac_promisc_set(hw, id, true);
+	/* Accept all: Multicast + Unicast */
+	mv_pp2x_prs_mac_uc_promisc_set(hw, id, true);
+	mv_pp2x_prs_mac_mc_promisc_set(hw, id, true);
 	/* Remove all port->id's mcast enries */
 	mv_pp2x_prs_mac_entry_del(port, MVPP2_PRS_MAC_MC, MVPP2_DEL_MAC_ALL);
 	/* Remove all port->id's ucast enries except M2M entry */
@@ -3774,14 +4113,22 @@
 	int id = port->id;
 
 	/* Accept all multicast */
-	mv_pp2x_prs_mac_multi_set(hw, id,
-				  MVPP2_PE_MAC_MC_ALL, true);
-	mv_pp2x_prs_mac_multi_set(hw, id,
-				  MVPP2_PE_MAC_MC_IP6, true);
+	mv_pp2x_prs_mac_mc_promisc_set(hw, id, true);
 	/* Remove all multicast filter entries from parser */
 	mv_pp2x_prs_mac_entry_del(port, MVPP2_PRS_MAC_MC, MVPP2_DEL_MAC_ALL);
 }
 
+static void mv_pp2x_set_rx_uc_multi(struct mv_pp2x_port *port)
+{
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	int id = port->id;
+
+	/* Accept all unicast */
+	mv_pp2x_prs_mac_uc_promisc_set(hw, id, true);
+	/* Remove all unicast filter entries from parser */
+	mv_pp2x_prs_mac_entry_del(port, MVPP2_PRS_MAC_UC, MVPP2_DEL_MAC_ALL);
+}
+
 /* register unicast and multicast addresses */
 static void mv_pp2x_set_rx_mode(struct net_device *dev)
 {
@@ -3794,28 +4141,28 @@
 	if (dev->flags & IFF_PROMISC) {
 		mv_pp2x_set_rx_promisc(port);
 	} else {
-		/* Put dev into promisc if MAC num greater than uc filter max */
+		/* Put dev into UC promisc if MAC num greater than uc filter max */
 		if (netdev_uc_count(dev) > port->priv->pp2_cfg.uc_filter_max) {
-			mv_pp2x_set_rx_promisc(port);
-			return;
-		}
-		/* Remove old enries not in uc list except M2M entry */
-		mv_pp2x_prs_mac_entry_del(port,
-					  MVPP2_PRS_MAC_UC,
-					  MVPP2_DEL_MAC_NOT_IN_LIST);
-		/* Add all entries into to uc mac addr filter list */
-		netdev_for_each_uc_addr(ha, dev) {
-			err = mv_pp2x_prs_mac_da_accept(port,
-							ha->addr, true);
-			if (err)
-				netdev_err(dev,
-					   "[%2x:%2x:%2x:%2x:%2x:%x]add fail\n",
-					   ha->addr[0], ha->addr[1],
-					   ha->addr[2], ha->addr[3],
-					   ha->addr[4], ha->addr[5]);
+			mv_pp2x_set_rx_uc_multi(port);
+		} else {
+			/* Remove old enries not in uc list except M2M entry */
+			mv_pp2x_prs_mac_entry_del(port,
+						  MVPP2_PRS_MAC_UC,
+						  MVPP2_DEL_MAC_NOT_IN_LIST);
+			/* Add all entries into to uc mac addr filter list */
+			netdev_for_each_uc_addr(ha, dev) {
+				err = mv_pp2x_prs_mac_da_accept(port,
+								ha->addr, true);
+				if (err)
+					netdev_err(dev,
+						   "[%2x:%2x:%2x:%2x:%2x:%x]add fail\n",
+						   ha->addr[0], ha->addr[1],
+						   ha->addr[2], ha->addr[3],
+						   ha->addr[4], ha->addr[5]);
+			}
+			/* Leave promisc mode */
+			mv_pp2x_prs_mac_uc_promisc_set(hw, id, false);
 		}
-		/* Leave promisc mode */
-		mv_pp2x_prs_mac_promisc_set(hw, id, false);
 
 		if (dev->flags & IFF_ALLMULTI) {
 			mv_pp2x_set_rx_allmulti(port);
@@ -3846,17 +4193,13 @@
 				}
 			}
 			/* Reject other MC mac entries */
-			mv_pp2x_prs_mac_multi_set(hw, id,
-						  MVPP2_PE_MAC_MC_ALL, false);
-			mv_pp2x_prs_mac_multi_set(hw, id,
-						  MVPP2_PE_MAC_MC_IP6, false);
+			mv_pp2x_prs_mac_mc_promisc_set(hw, id, false);
 		}
 	}
 }
 
 static int mv_pp2x_set_mac_address(struct net_device *dev, void *p)
 {
-	struct mv_pp2x_port *port = netdev_priv(dev);
 	const struct sockaddr *addr = p;
 	int err;
 
@@ -3865,28 +4208,11 @@
 		goto error;
 	}
 
-	if (!netif_running(dev)) {
-		err = mv_pp2x_prs_update_mac_da(dev, addr->sa_data);
-		if (!err)
-			return 0;
-		/* Reconfigure parser to accept the original MAC address */
-		err = mv_pp2x_prs_update_mac_da(dev, dev->dev_addr);
-		goto error;
-	}
-
-	mv_pp2x_stop_dev(port);
-
 	err = mv_pp2x_prs_update_mac_da(dev, addr->sa_data);
 	if (!err)
-		goto out_start;
-
-	/* Reconfigure parser accept the original MAC address */
+		return 0;
+	/* Reconfigure parser to accept the original MAC address */
 	err = mv_pp2x_prs_update_mac_da(dev, dev->dev_addr);
-	if (err)
-		goto error;
-out_start:
-	mv_pp2x_start_dev(port);
-	return 0;
 
 error:
 	netdev_err(dev, "fail to change MAC address\n");
@@ -4065,19 +4391,36 @@
 			 void *accel_priv, select_queue_fallback_t fallback)
 
 {
-	int val;
-
 	/* If packet in coming from Rx -> RxQ = TxQ, callback function used for packets from CPU Tx */
 	if (skb->queue_mapping)
-		val = skb->queue_mapping - 1;
+		return ((skb->queue_mapping - 1) % mv_pp2x_txq_number) + (smp_processor_id() * mv_pp2x_txq_number);
 	else
-		val = fallback(dev, skb);
+		return mv_pp2x_txq_number * fallback(dev, skb);
+}
 
-	return (val % mv_pp2x_txq_number) + (smp_processor_id() * mv_pp2x_txq_number);
+/* Dummy netdev_ops for non-kernel (i.e. musdk) network devices */
+static int mv_pp2x_dummy_change_mtu(struct net_device *dev, int mtu)
+{
+	netdev_warn(dev, "ndo_change_mtu not supported\n");
+	return 0;
 }
 
-/* Device ops */
+int mv_pp2x_dummy_stop(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
 
+	port->flags |= MVPP2_F_IF_MUSDK_DOWN;
+	netdev_warn(dev, "ndo_stop not supported\n");
+	return 0;
+}
+
+static int mv_pp2x_dummy_tx(struct sk_buff *skb, struct net_device *dev)
+{
+	pr_debug("mv_pp2x_dummy_tx\n");
+	return NETDEV_TX_OK;
+}
+
+/* Device ops */
 static const struct net_device_ops mv_pp2x_netdev_ops = {
 	.ndo_open		= mv_pp2x_open,
 	.ndo_stop		= mv_pp2x_stop,
@@ -4093,6 +4436,22 @@
 	.ndo_vlan_rx_kill_vid	= mv_pp2x_rx_kill_vid,
 };
 
+/* musdk ports contain dummy operations for those functions that are performed in UserSpace (i.e. musdk) */
+static const struct net_device_ops mv_pp2x_non_kernel_netdev_ops = {
+	.ndo_open		= mv_pp2x_open,
+	.ndo_stop		= mv_pp2x_dummy_stop,
+	.ndo_start_xmit		= mv_pp2x_dummy_tx,
+	/*.ndo_select_queue	= mv_pp2x_select_queue,*/
+	.ndo_set_rx_mode	= mv_pp2x_set_rx_mode,
+	.ndo_set_mac_address	= mv_pp2x_set_mac_address,
+	.ndo_change_mtu		= mv_pp2x_dummy_change_mtu,
+	.ndo_get_stats64	= mv_pp2x_get_stats64,
+	.ndo_do_ioctl		= mv_pp2x_ioctl,
+	.ndo_set_features	= mv_pp2x_netdev_set_features,
+	.ndo_vlan_rx_add_vid	= mv_pp2x_rx_add_vid,
+	.ndo_vlan_rx_kill_vid	= mv_pp2x_rx_kill_vid,
+};
+
 /* Driver initialization */
 
 static void mv_pp21_port_power_up(struct mv_pp2x_port *port)
@@ -4317,6 +4676,8 @@
 		fixed_link_node = of_get_child_by_name(emac_node, "fixed-link");
 		port->mac_data.duplex = of_property_read_bool(fixed_link_node,
 				"full-duplex");
+		if (port->mac_data.speed == SPEED_2500)
+			port->mac_data.flags |= MV_EMAC_F_SGMII2_5;
 		if (of_property_read_u32(fixed_link_node, "speed",
 					 &port->mac_data.speed))
 			return -EINVAL;
@@ -4431,12 +4792,12 @@
 
 	if (port->priv->pp2_version == PPV21)
 		return;
-
-	link_is_up = mv_gop110_port_is_link_up(gop, &port->mac_data);
-
-	if (link_is_up) {
-		mv_gop110_mib_counters_stat_update(gop, gop_port, gop_statistics);
-		mv_pp2x_counters_stat_update(port, gop_statistics);
+	if (!(port->flags & MVPP2_F_LOOPBACK)) {
+		link_is_up = mv_gop110_port_is_link_up(gop, &port->mac_data);
+		if (link_is_up) {
+			mv_gop110_mib_counters_stat_update(gop, gop_port, gop_statistics);
+			mv_pp2x_counters_stat_update(port, gop_statistics);
+		}
 	}
 }
 
@@ -4456,13 +4817,12 @@
 }
 
 /* Initialize port HW */
-static int mv_pp2x_port_init(struct mv_pp2x_port *port)
+static int mv_pp2x_port_hw_init(struct mv_pp2x_port *port)
 {
-	struct device *dev = port->dev->dev.parent;
 	struct mv_pp2x *priv = port->priv;
 	struct gop_hw *gop = &port->priv->hw.gop;
 	struct mv_mac_data *mac = &port->mac_data;
-	int queue, err;
+	int err = 0;
 
 	/* Disable port */
 	mv_pp2x_egress_disable(port);
@@ -4470,7 +4830,33 @@
 	if (port->priv->pp2_version == PPV21)
 		mv_pp21_port_disable(port);
 	else
-		mv_gop110_port_disable(gop, mac, port->comphy);
+		if (!(port->flags & MVPP2_F_LOOPBACK))
+			mv_gop110_port_disable(gop, mac, port->comphy);
+
+	/* Configure Rx queue group interrupt for this port */
+	priv->pp2xdata->mv_pp2x_port_isr_rx_group_cfg(port);
+
+	mv_pp2x_ingress_disable(port);
+
+	/* Port default configuration */
+	mv_pp2x_defaults_set(port);
+
+	/* Port's classifier configuration */
+	mv_pp2x_cls_oversize_rxq_set(port);
+	mv_pp2x_cls_port_config(port);
+
+	/* Initialize pools for swf */
+	if (!(port->flags & MVPP2_F_IF_MUSDK))
+		err = mv_pp2x_swf_bm_pool_init(port);
+
+	return err;
+}
+
+/* Initialize port */
+static int mv_pp2x_port_init(struct mv_pp2x_port *port)
+{
+	struct device *dev = port->dev->dev.parent;
+	int queue, err;
 
 	/* Allocate queues */
 	port->txqs = devm_kcalloc(dev, port->num_tx_queues, sizeof(*port->txqs),
@@ -4490,16 +4876,9 @@
 
 	/* Associate physical Rx queues to port and initialize.  */
 	err = mv_pp2x_port_rxqs_init(dev, port);
-
 	if (err)
 		goto err_free_percpu;
 
-	/* Configure queue_vectors */
-	priv->pp2xdata->mv_pp2x_port_queue_vectors_init(port);
-
-	/* Configure Rx queue group interrupt for this port */
-	priv->pp2xdata->mv_pp2x_port_isr_rx_group_cfg(port);
-
 	/* Create Rx descriptor rings */
 	for (queue = 0; queue < port->num_rx_queues; queue++) {
 		struct mv_pp2x_rx_queue *rxq = port->rxqs[queue];
@@ -4509,20 +4888,14 @@
 		rxq->time_coal = MVPP2_RX_COAL_USEC;
 	}
 
-	mv_pp2x_ingress_disable(port);
-
-	/* Port default configuration */
-	mv_pp2x_defaults_set(port);
-
-	/* Port's classifier configuration */
-	mv_pp2x_cls_oversize_rxq_set(port);
-	mv_pp2x_cls_port_config(port);
-
 	/* Provide an initial Rx packet size */
 	port->pkt_size = MVPP2_RX_PKT_SIZE(port->dev->mtu);
 
-	/* Initialize pools for swf */
-	err = mv_pp2x_swf_bm_pool_init(port);
+	/* Configure queue_vectors */
+	if (!(port->flags & MVPP2_F_IF_MUSDK))
+		port->priv->pp2xdata->mv_pp2x_port_queue_vectors_init(port);
+
+	err = mv_pp2x_port_hw_init(port);
 	if (err)
 		goto err_free_percpu;
 	return 0;
@@ -4562,6 +4935,8 @@
 	struct queue_vector *qvec;
 	cpumask_t cpus_mask;
 	struct mv_pp2x_port *port = container_of(nfb, struct mv_pp2x_port, port_hotplug_nb);
+	struct mv_pp2x_aggr_tx_queue *aggr_txq;
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
 
 	switch (action) {
 	case CPU_ONLINE:
@@ -4583,6 +4958,14 @@
 		if (port->priv->pp2xdata->interrupt_tx_done)
 			on_each_cpu_mask(&cpus_mask, mv_pp2x_tx_done_pkts_coal_set, port, 1);
 		break;
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		cp_pcpu = per_cpu_ptr(port->priv->pcpu, cpu);
+		aggr_txq = &port->priv->aggr_txqs[cpu];
+		mv_pp2x_tx_timer_kill(cp_pcpu);
+		aggr_txq->hw_count += aggr_txq->sw_count;
+		mv_pp22_thread_write(&port->priv->hw, cpu, MVPP2_AGGR_TXQ_UPDATE_REG, aggr_txq->sw_count);
+		aggr_txq->sw_count = 0;
 	}
 
 	return NOTIFY_OK;
@@ -4593,15 +4976,15 @@
 			      struct device_node *port_node,
 			    struct mv_pp2x *priv)
 {
-	struct device_node *emac_node;
-	struct device_node *phy_node;
+	struct device_node *emac_node = NULL;
+	struct device_node *phy_node = NULL;
 	struct mv_pp2x_port *port;
 	struct mv_pp2x_port_pcpu *port_pcpu;
 	struct net_device *dev;
 	struct resource *res;
-	const char *dt_mac_addr;
+	const char *dt_mac_addr = NULL;
 	const char *mac_from;
-	char hw_mac_addr[ETH_ALEN];
+	char hw_mac_addr[ETH_ALEN] = {0};
 	u32 id;
 	int features, err = 0, i, cpu;
 	int priv_common_regs_num = 2;
@@ -4609,18 +4992,38 @@
 	unsigned int *port_irqs;
 	int port_num_irq;
 	int phy_mode;
-	struct phy *comphy;
-
-	dev = alloc_etherdev_mqs(sizeof(struct mv_pp2x_port),
-				 mv_pp2x_txq_number * num_active_cpus(), mv_pp2x_rxq_number);
+	struct phy *comphy = NULL;
+	const char *musdk_status;
+	int statlen;
+
+	if (of_property_read_bool(port_node, "marvell,loopback")) {
+		dev = alloc_netdev_mqs(sizeof(struct mv_pp2x_port), "pp2_lpbk%d", NET_NAME_UNKNOWN,
+				       ether_setup, mv_pp2x_txq_number * num_active_cpus(), mv_pp2x_rxq_number);
+	} else {
+		dev = alloc_etherdev_mqs(sizeof(struct mv_pp2x_port),
+					 mv_pp2x_txq_number * num_active_cpus(), mv_pp2x_rxq_number);
+	}
 	if (!dev)
 		return -ENOMEM;
 
+	/* Setup XPS mapping */
+	for_each_present_cpu(cpu) {
+		cpumask_set_cpu(cpu, &priv->aggr_txqs[cpu].affinity_mask);
+		netif_set_xps_queue(dev, &priv->aggr_txqs[cpu].affinity_mask, cpu);
+	}
+
 	/*Connect entities */
 	port = netdev_priv(dev);
 	port->dev = dev;
 	SET_NETDEV_DEV(dev, &pdev->dev);
 	port->priv = priv;
+	port->flags = 0;
+
+	musdk_status = of_get_property(port_node, "musdk-status", &statlen);
+
+	/* Set musdk_flag, only if status is "private", not if status is "shared" */
+	if (musdk_status && !strcmp(musdk_status, "private"))
+		port->flags |= MVPP2_F_IF_MUSDK;
 
 	mv_pp2x_port_init_config(port);
 
@@ -4649,20 +5052,27 @@
 		port->mac_data.phy_node = phy_node;
 		emac_node = port_node;
 	} else {
-		emac_node = of_parse_phandle(port_node, "emac-data", 0);
-		if (!emac_node) {
-			dev_err(&pdev->dev, "missing emac-data\n");
-			err = -EINVAL;
-			goto err_free_netdev;
-		}
-		/* Init emac_data, includes link interrupt */
-		if (mv_pp2_init_emac_data(port, emac_node))
-			goto err_free_netdev;
+		if (of_property_read_bool(port_node, "marvell,loopback"))
+			port->flags |= MVPP2_F_LOOPBACK;
+
+		if (!(port->flags & MVPP2_F_LOOPBACK)) {
+			emac_node = of_parse_phandle(port_node, "emac-data", 0);
+			if (!emac_node) {
+				dev_err(&pdev->dev, "missing emac-data\n");
+				err = -EINVAL;
+				goto err_free_netdev;
+			}
+			/* Init emac_data, includes link interrupt */
+			if (mv_pp2_init_emac_data(port, emac_node))
+				goto err_free_netdev;
 
-		comphy = devm_of_phy_get(&pdev->dev, emac_node, "comphy");
+			comphy = devm_of_phy_get(&pdev->dev, emac_node, "comphy");
 
-		if (!IS_ERR(comphy))
-			port->comphy = comphy;
+			if (!IS_ERR(comphy))
+				port->comphy = comphy;
+		} else {
+			port->mac_data.link_irq = MVPP2_NO_LINK_IRQ;
+		}
 	}
 
 	if (port->mac_data.phy_node) {
@@ -4672,7 +5082,9 @@
 	}
 
 	/* get MAC address */
-	dt_mac_addr = of_get_mac_address(emac_node);
+	if (emac_node)
+		dt_mac_addr = of_get_mac_address(emac_node);
+
 	if (dt_mac_addr && is_valid_ether_addr(dt_mac_addr)) {
 		mac_from = "device tree";
 		ether_addr_copy(dev->dev_addr, dt_mac_addr);
@@ -4694,7 +5106,9 @@
 
 	/* Tx/Rx Interrupt */
 	port_num_irq = mv_pp2x_of_irq_count(port_node);
-	if (port_num_irq != priv->pp2xdata->num_port_irq) {
+	if (port->flags & MVPP2_F_IF_MUSDK)
+		port_num_irq = 0;
+	if ((!(port->flags & MVPP2_F_IF_MUSDK)) && port_num_irq != priv->pp2xdata->num_port_irq) {
 		dev_err(&pdev->dev,
 			"port(%d)-number of irq's doesn't match hw\n", id);
 		goto err_free_netdev;
@@ -4714,16 +5128,20 @@
 		port->num_irqs++;
 	}
 
-	/*FIXME, full handling loopback */
-	if (of_property_read_bool(port_node, "marvell,loopback"))
-		port->flags |= MVPP2_F_LOOPBACK;
-
-	port->num_tx_queues = mv_pp2x_txq_number;
-	port->num_rx_queues = mv_pp2x_rxq_number;
 	dev->tx_queue_len = tx_queue_size;
 	dev->watchdog_timeo = 5 * HZ;
-	dev->netdev_ops = &mv_pp2x_netdev_ops;
-	mv_pp2x_set_ethtool_ops(dev);
+
+	if (port->flags & MVPP2_F_IF_MUSDK) {
+		port->num_tx_queues = 0;
+		port->num_rx_queues = 0;
+		dev->netdev_ops = &mv_pp2x_non_kernel_netdev_ops;
+		mv_pp2x_set_non_kernel_ethtool_ops(dev);
+	} else {
+		port->num_tx_queues = mv_pp2x_txq_number;
+		port->num_rx_queues = mv_pp2x_rxq_number;
+		dev->netdev_ops = &mv_pp2x_netdev_ops;
+		mv_pp2x_set_ethtool_ops(dev);
+	}
 
 	if (priv->pp2_version == PPV21)
 		port->first_rxq = (port->id) * mv_pp2x_rxq_number +
@@ -4755,7 +5173,8 @@
 	mv_pp2x_check_queue_size_valid(port);
 
 	if (mv_pp2_num_cpu_irqs(port) < num_active_cpus() &&
-	    port->priv->pp2xdata->interrupt_tx_done) {
+	    port->priv->pp2xdata->interrupt_tx_done &&
+	    (!(port->flags & (MVPP2_F_IF_MUSDK | MVPP2_F_LOOPBACK)))) {
 		port->priv->pp2xdata->interrupt_tx_done = false;
 		dev_info(&pdev->dev, "mvpp2x: interrupt_tx_done override to false\n");
 	}
@@ -4773,7 +5192,7 @@
 		err = -ENOMEM;
 		goto err_free_txq_pcpu;
 	}
-	if (!port->priv->pp2xdata->interrupt_tx_done) {
+	if ((!(port->flags & (MVPP2_F_IF_MUSDK | MVPP2_F_LOOPBACK))) && !port->priv->pp2xdata->interrupt_tx_done) {
 		for_each_present_cpu(cpu) {
 			port_pcpu = per_cpu_ptr(port->pcpu, cpu);
 
@@ -4786,6 +5205,9 @@
 				     mv_pp2x_tx_proc_cb, (unsigned long)dev);
 		}
 	}
+
+	if (port->flags & MVPP2_F_IF_MUSDK)
+		goto skip_tso_buffers;
 	/* Init pool of external buffers for TSO, fragmentation, etc */
 	for_each_present_cpu(cpu) {
 		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
@@ -4815,6 +5237,7 @@
 		}
 	}
 
+skip_tso_buffers:
 	features = NETIF_F_SG;
 	dev->features = features | NETIF_F_RXCSUM | NETIF_F_IP_CSUM |
 			NETIF_F_IPV6_CSUM | NETIF_F_TSO;
@@ -4864,6 +5287,9 @@
 	mv_pp2x_ptp_init(pdev, port, id);
 #endif
 
+	/* Populate network device of_node */
+	dev->dev.of_node = port_node;
+
 	return 0;
 	dev_err(&pdev->dev, "%s failed for port_id(%d)\n", __func__, id);
 
@@ -5025,13 +5451,27 @@
 		       hw->lms_base + MVPP2_MNG_EXTENDED_GLOBAL_CTRL_REG);
 	}
 
-	/* Allocate and initialize aggregated TXQs */
-	priv->aggr_txqs = devm_kcalloc(&pdev->dev, num_active_cpus(),
-				       sizeof(struct mv_pp2x_aggr_tx_queue),
-				       GFP_KERNEL);
-
+	/* Allocate and initialize aggregated TXQs
+	 * The aggr_txqs area should be aligned onto cache-line-size.
+	 * So allocate cache-line-size more than needed, round-up the pointer
+	 * but keep the offset between aligned and original pointers
+	 * for further usage in free(aligned - offset).
+	 * (offset is used instead of origin-ptr since it is more compact)
+	 */
+	val = sizeof(struct mv_pp2x_aggr_tx_queue) * num_active_cpus() +
+		MVPP2_CACHE_LINE_SIZE;
+	priv->aggr_txqs = devm_kcalloc(&pdev->dev, 1, val, GFP_KERNEL);
 	if (!priv->aggr_txqs)
 		return -ENOMEM;
+	val = (dma_addr_t)priv->aggr_txqs & MVPP2_CACHE_LINE_MASK;
+	if (!val) {
+		priv->aggr_txqs_align_offs = 0;
+	} else {
+		priv->aggr_txqs_align_offs = sizeof(struct mv_pp2x_aggr_tx_queue) - val;
+		priv->aggr_txqs = (void *)((u8 *)priv->aggr_txqs +
+			priv->aggr_txqs_align_offs);
+	}
+
 	priv->num_aggr_qs = num_active_cpus();
 
 	i = 0;
@@ -5418,15 +5858,16 @@
 			priv->l4_chksum_jumbo_port = priv->port_list[0]->id;
 	}
 
-	/* Set FIFO according to l4_chksum_jumbo_port */
-	for (i = 0; i < priv->num_ports; i++) {
-		if (priv->port_list[i]->id != priv->l4_chksum_jumbo_port) {
-			mv_pp2x_tx_fifo_size_set(&priv->hw,
-						 priv->port_list[i]->id,
-					    MVPP2_TX_FIFO_DATA_SIZE_3KB);
-			mv_pp2x_tx_fifo_threshold_set(&priv->hw,
-						      priv->port_list[i]->id,
-					    MVPP2_TX_FIFO_THRESHOLD_3KB);
+	/* Set FIFO according to l4_chksum_jumbo_port.
+	*  10KB for l4_chksum_jumbo_port and 3KB for other ports.
+	*  TX FIFO should be set for all ports, even if port not initialized.
+	*/
+	for (i = 0; i < MVPP2_MAX_PORTS; i++) {
+		if (i != priv->l4_chksum_jumbo_port) {
+			mv_pp2x_tx_fifo_size_set(&priv->hw, i,
+						 MVPP2_TX_FIFO_DATA_SIZE_3KB);
+			mv_pp2x_tx_fifo_threshold_set(&priv->hw, i,
+						      MVPP2_TX_FIFO_THRESHOLD_3KB);
 			}
 	}
 	mv_pp2x_tx_fifo_size_set(&priv->hw,
@@ -5521,11 +5962,13 @@
 	u32 cell_index = 0;
 	struct device_node *dn = pdev->dev.of_node;
 	struct device_node *port_node;
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
 
 	priv = devm_kzalloc(&pdev->dev, sizeof(struct mv_pp2x), GFP_KERNEL);
 	if (!priv)
 		return -ENOMEM;
 	hw = &priv->hw;
+	priv->pdev = pdev;
 
 	err = mv_pp2x_platform_data_get(pdev, priv, &cell_index, &port_count);
 	if (err) {
@@ -5580,6 +6023,25 @@
 		goto err_clk;
 	}
 
+	priv->pcpu = alloc_percpu(struct mv_pp2x_cp_pcpu);
+	if (!priv->pcpu) {
+		err = -ENOMEM;
+		goto  err_clk;
+	}
+
+	/* Init per CPU CP skb list for skb recycling */
+	for_each_present_cpu(cpu) {
+		cp_pcpu = per_cpu_ptr(priv->pcpu, cpu);
+
+		INIT_LIST_HEAD(&cp_pcpu->skb_port_list);
+		cp_pcpu->skb_pool = devm_kzalloc(&pdev->dev,
+			sizeof(struct mv_pp2x_skb_pool), GFP_ATOMIC);
+		cp_pcpu->skb_pool->skb_pool_size = MVPP2_SKB_NUM;
+		cp_pcpu->skb_pool->skb_struct =
+			devm_kzalloc(&pdev->dev,
+				     sizeof(struct mv_pp2x_skb_struct) * MVPP2_SKB_NUM, GFP_ATOMIC);
+	}
+
 	/* Init PP22 rxfhindir table evenly in probe */
 	if (priv->pp2_version == PPV22) {
 		mv_pp22_init_rxfhindir(priv);
@@ -5592,6 +6054,22 @@
 		if (err < 0)
 			goto err_clk;
 	}
+	/* Init hrtimer for tx transmit procedure.
+	 * Instead of reg_write atfer each xmit callback, 50 microsecond
+	 * hrtimer would be started. Hrtimer will reduce amount of accesses
+	 * to transmit register and dmb() influence on network performance.
+	 */
+	for_each_present_cpu(cpu) {
+		cp_pcpu = per_cpu_ptr(priv->pcpu, cpu);
+
+		hrtimer_init(&cp_pcpu->tx_timer, CLOCK_MONOTONIC,
+			     HRTIMER_MODE_REL_PINNED);
+		cp_pcpu->tx_timer.function = mv_pp2x_tx_hr_timer_cb;
+		cp_pcpu->tx_timer_scheduled = false;
+
+		tasklet_init(&cp_pcpu->tx_tasklet,
+			     mv_pp2x_tx_send_proc_cb, (unsigned long)priv);
+	}
 
 	if (priv->pp2_version == PPV22) {
 		/* Init tx&rx fifo for each port */
@@ -5638,7 +6116,8 @@
 {
 	struct mv_pp2x *priv = platform_get_drvdata(pdev);
 	struct mv_pp2x_hw *hw = &priv->hw;
-	int i, num_of_ports;
+	int i, num_of_ports, cpu;
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
 
 	if (priv->pp2_version == PPV22 && mv_pp2x_queue_mode == MVPP2_QDIST_MULTI_MODE)
 		unregister_hotcpu_notifier(&priv->cp_hotplug_nb);
@@ -5647,6 +6126,11 @@
 	flush_workqueue(priv->workqueue);
 	destroy_workqueue(priv->workqueue);
 
+	for_each_present_cpu(cpu) {
+		cp_pcpu = per_cpu_ptr(priv->pcpu, cpu);
+		tasklet_kill(&cp_pcpu->tx_tasklet);
+	}
+
 	num_of_ports = priv->num_ports;
 
 	for (i = 0; i < num_of_ports; i++) {
@@ -5679,6 +6163,152 @@
 	return 0;
 }
 
+#ifdef CONFIG_PM_SLEEP
+/* Ports initialization after suspend */
+static int mv_pp2x_port_resume(struct platform_device *pdev,
+			       struct device_node *port_node,
+			       struct mv_pp2x *priv, struct mv_pp2x_port *port)
+{
+	struct device_node *emac_node = NULL;
+	int i;
+
+	if (priv->pp2_version == PPV22 && !(port->flags & MVPP2_F_LOOPBACK)) {
+		emac_node = of_parse_phandle(port_node, "emac-data", 0);
+		port->mac_data.link_irq = irq_of_parse_and_map(emac_node, 0);
+	}
+
+	if (port->mac_data.phy_node)
+		mv_pp2x_phy_connect(port);
+
+	for (i = 0; i < port->num_irqs; i++)
+		port->of_irqs[i] = irq_of_parse_and_map(port_node, i);
+
+	mv_pp2x_port_hw_init(port);
+
+	return 0;
+}
+
+/* Routine configure mvpp2x HW after suspend */
+static int mv_pp2x_probe_after_suspend(struct device *dev)
+{
+	struct mv_pp2x *priv = dev_get_drvdata(dev);
+	int i, err;
+	struct platform_device *pdev = priv->pdev;
+	struct device_node *dn = pdev->dev.of_node;
+	struct device_node *port_node;
+
+	/* Resume network controller */
+	err = mv_pp2x_init(pdev, priv);
+	if (err < 0) {
+		dev_err(&pdev->dev, "failed to resume controller\n");
+		return err;
+	}
+
+	i = 0;
+	/* Resume ports */
+	for_each_available_child_of_node(dn, port_node) {
+		mv_pp2x_port_resume(pdev, port_node, priv, priv->port_list[i]);
+		i++;
+	}
+
+	if (priv->pp2_version == PPV22) {
+		/* Init tx&rx fifo for each port */
+		mv_pp22_tx_fifo_init(priv);
+		mv_pp22_rx_fifo_init(priv);
+		mv_pp22_set_net_comp(priv);
+	} else {
+		mv_pp21_fifo_init(priv);
+	}
+
+	return 0;
+}
+
+/* Routine suspend to RAM CP */
+static int mvpp2x_suspend(struct device *dev)
+{
+	struct mv_pp2x *priv = dev_get_drvdata(dev);
+	int i, num_of_ports;
+	struct platform_device *pdev = priv->pdev;
+
+	num_of_ports = priv->num_ports;
+	for (i = 0; i < num_of_ports; i++) {
+		struct mv_mac_data *mac = &priv->port_list[i]->mac_data;
+		/* Stop interface if port is up */
+		if (netif_running(priv->port_list[i]->dev))
+			mv_pp2x_stop(priv->port_list[i]->dev);
+		/* Dispose all port IRQ's */
+		if (mac->link_irq != MVPP2_NO_LINK_IRQ)
+			irq_dispose_mapping(mac->link_irq);
+		mv_pp2x_port_irqs_dispose_mapping(priv->port_list[i]);
+
+		if (mac->phy_node)
+			mv_pp2x_phy_disconnect(priv->port_list[i]);
+
+		/* Null all pointers to BM pools, pool's will be reconfigured in resume procedure */
+		priv->port_list[i]->pool_short = NULL;
+		priv->port_list[i]->pool_long = NULL;
+		/* Missing flag will trigger GoP reconfiguration in resume routine */
+		mac->flags &= ~MV_EMAC_F_INIT;
+	}
+
+	for_each_present_cpu(i) {
+		struct mv_pp2x_aggr_tx_queue *aggr_txq = &priv->aggr_txqs[i];
+
+		dma_free_coherent(&pdev->dev,
+				  MVPP2_DESCQ_MEM_SIZE(aggr_txq->size),
+				  aggr_txq->desc_mem, aggr_txq->descs_phys);
+	}
+
+	devm_kfree(&pdev->dev, priv->hw.prs_shadow);
+	devm_kfree(&pdev->dev, priv->hw.cls_shadow);
+	devm_kfree(&pdev->dev, priv->hw.c2_shadow);
+	/* aggr_txqs is aligned round-up; restore the original by -offset */
+	devm_kfree(&pdev->dev, ((u8 *)priv->aggr_txqs - priv->aggr_txqs_align_offs));
+
+	for (i = 0; i < priv->num_pools; i++) {
+		struct mv_pp2x_bm_pool *bm_pool = &priv->bm_pools[i];
+
+		mv_pp2x_bm_pool_destroy(dev, priv, bm_pool);
+	}
+
+	return 0;
+}
+
+/* Routine resume CP after S2RAM */
+static int mvpp2x_resume(struct device *dev)
+{
+	struct mv_pp2x *priv;
+	int i, num_of_ports, err;
+
+	err = mv_pp2x_probe_after_suspend(dev);
+	if (err < 0)
+		return err;
+
+	priv = dev_get_drvdata(dev);
+	num_of_ports = priv->num_ports;
+
+	for (i = 0; i < num_of_ports; i++) {
+		if (priv->port_list[i]->comphy)
+			phy_init(priv->port_list[i]->comphy);
+		/* Start interface if port was up before suspend */
+		if (netif_running(priv->port_list[i]->dev))
+			mv_pp2x_open(priv->port_list[i]->dev);
+
+		if (priv->port_list[i]->dev->flags & IFF_PROMISC)
+			mv_pp2x_set_rx_promisc(priv->port_list[i]);
+		else if (priv->port_list[i]->dev->flags & IFF_ALLMULTI)
+			mv_pp2x_set_rx_allmulti(priv->port_list[i]);
+	}
+
+	return 0;
+}
+
+static const struct dev_pm_ops mv_pp2x_pm_ops = {
+	.suspend = mvpp2x_suspend,
+	.resume = mvpp2x_resume,
+};
+#endif /* CONFIG_PM_SLEEP */
+
 MODULE_DEVICE_TABLE(of, mv_pp2x_match_tbl);
 
 static struct platform_driver mv_pp2x_driver = {
@@ -5687,6 +6317,9 @@
 	.driver = {
 		.name = MVPP2_DRIVER_NAME,
 		.of_match_table = mv_pp2x_match_tbl,
+#ifdef CONFIG_PM_SLEEP
+		.pm = &mv_pp2x_pm_ops,
+#endif
 	},
 };
 
